{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_quantization import tensor_quant\n",
    "\n",
    "from torch import nn, outer\n",
    "\n",
    "from pytorch_quantization import tensor_quant\n",
    "import pytorch_quantization.nn as quant_nn\n",
    "import pytorch_quantization\n",
    "pytorch_quantization.__version__\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.yolo import YOLO\n",
    "from engine.detector import Detector\n",
    "from model.squeezedet import SqueezeDet, SqueezeDetWithLoss\n",
    "from utils.config import Config\n",
    "from utils.model import load_model\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "from utils.misc import init_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arch                           mobilenet_v2\n",
      "batch_size                     1\n",
      "bbox_loss_weight               20.0\n",
      "chunk_sizes                    [32]\n",
      "class_loss_weight              1.0\n",
      "data_dir                       /media/gj_data/QAT_Squeezedet/SqueezeDet-PyTorch/data\n",
      "dataset                        lpr\n",
      "debug                          2\n",
      "debug_dir                      /media/gj_data/QAT_Squeezedet/SqueezeDet-PyTorch/exp/test_lpd_trt_calib/debug\n",
      "device                         cuda\n",
      "drift_prob                     1.0\n",
      "dropout_prob                   0\n",
      "exp_dir                        /media/gj_data/QAT_Squeezedet/SqueezeDet-PyTorch/exp\n",
      "exp_id                         test_lpd_trt_calib\n",
      "flip_prob                      0.5\n",
      "forbid_resize                  False\n",
      "gpus                           [0]\n",
      "gpus_str                       0\n",
      "grad_norm                      0.5\n",
      "keep_top_k                     64\n",
      "load_model                     ../exp/exp14_128_mobv2_stride8/model_best.pth\n",
      "load_pretrained                False\n",
      "log_file                       /media/gj_data/QAT_Squeezedet/SqueezeDet-PyTorch/exp/test_lpd_trt_calib/training_logs.txt\n",
      "lr                             0.001\n",
      "master_batch_size              32\n",
      "mode                           eval\n",
      "momentum                       0.9\n",
      "negative_score_loss_weight     100.0\n",
      "nms_thresh                     0.4\n",
      "no_eval                        False\n",
      "not_cuda_benchmark             False\n",
      "num_epochs                     100\n",
      "num_iters                      -1\n",
      "num_workers                    0\n",
      "positive_score_loss_weight     3.75\n",
      "print_interval                 20\n",
      "qat                            False\n",
      "root_dir                       /media/gj_data/QAT_Squeezedet/SqueezeDet-PyTorch\n",
      "save_dir                       /media/gj_data/QAT_Squeezedet/SqueezeDet-PyTorch/exp/test_lpd_trt_calib\n",
      "save_intervals                 1\n",
      "score_thresh                   0.5\n",
      "seed                           42\n",
      "stride                         8\n",
      "val_intervals                  1\n",
      "weight_decay                   0.0001\n"
     ]
    }
   ],
   "source": [
    "args = {'_': '_'}\n",
    "cfg = Config().parse(args)\n",
    "\n",
    "init_env(cfg) \n",
    " \n",
    "cfg.mode = 'eval'\n",
    "cfg.exp_id = \"test_lpd_trt_calib\"\n",
    "cfg.dataset = 'lpr'\n",
    "cfg.load_model = '../exp/exp14_128_mobv2_stride8/model_best.pth' \n",
    "cfg.batch_size = 1\n",
    "cfg.arch = 'mobilenet_v2'\n",
    "cfg.num_workers = 0\n",
    "cfg.save_intervals = 1\n",
    "cfg.stride = 8\n",
    "cfg.debug = 2\n",
    "\n",
    "def update_exp_dir(cfg, exp_id):\n",
    "    cfg.save_dir = os.path.join(cfg.exp_dir, exp_id)\n",
    "    os.makedirs(cfg.save_dir, exist_ok=True)\n",
    "    cfg.log_file = os.path.join(cfg.save_dir, 'training_logs.txt')\n",
    "    os.remove(cfg.log_file) if os.path.exists(cfg.log_file) else None\n",
    "    cfg.debug_dir = os.path.join(cfg.save_dir, 'debug')\n",
    "\n",
    "update_exp_dir(cfg, cfg.exp_id)\n",
    "\n",
    "Config().print(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from eval import eval\n",
    "# eval(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine.detector import Detector\n",
    "from model.squeezedet import SqueezeDetWithLoss\n",
    "from utils.model import load_model\n",
    "from utils.misc import load_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchors                        [[  4.   4.   3.   3.]\n",
      " [  4.   4.   6.   5.]\n",
      " [  4.   4.   9.   5.]\n",
      " ...\n",
      " [124. 124.  10.   9.]\n",
      " [124. 124.  10.  12.]\n",
      " [124. 124.  15.   8.]]\n",
      "anchors_per_grid               6\n",
      "arch                           mobilenet_v2\n",
      "batch_size                     1\n",
      "bbox_loss_weight               20.0\n",
      "chunk_sizes                    [32]\n",
      "class_loss_weight              1.0\n",
      "class_names                    0\n",
      "data_dir                       /media/gj_data/QAT_Squeezedet/SqueezeDet-PyTorch/data\n",
      "dataset                        lpr\n",
      "debug                          2\n",
      "debug_dir                      /media/gj_data/QAT_Squeezedet/SqueezeDet-PyTorch/exp/test_lpd_trt_calib/debug\n",
      "device                         cuda\n",
      "drift_prob                     1.0\n",
      "dropout_prob                   0\n",
      "exp_dir                        /media/gj_data/QAT_Squeezedet/SqueezeDet-PyTorch/exp\n",
      "exp_id                         test_lpd_trt_calib\n",
      "flip_prob                      0.5\n",
      "forbid_resize                  False\n",
      "gpus                           [0]\n",
      "gpus_str                       0\n",
      "grad_norm                      0.5\n",
      "grid_size                      (16, 16)\n",
      "input_size                     (128, 128)\n",
      "keep_top_k                     64\n",
      "load_model                     ../exp/exp14_128_mobv2_stride8/model_best.pth\n",
      "load_pretrained                False\n",
      "log_file                       /media/gj_data/QAT_Squeezedet/SqueezeDet-PyTorch/exp/test_lpd_trt_calib/training_logs.txt\n",
      "lr                             0.001\n",
      "master_batch_size              32\n",
      "mode                           eval\n",
      "momentum                       0.9\n",
      "negative_score_loss_weight     100.0\n",
      "nms_thresh                     0.4\n",
      "no_eval                        False\n",
      "not_cuda_benchmark             False\n",
      "num_anchors                    1536\n",
      "num_classes                    1\n",
      "num_epochs                     100\n",
      "num_iters                      -1\n",
      "num_workers                    0\n",
      "positive_score_loss_weight     3.75\n",
      "print_interval                 20\n",
      "qat                            False\n",
      "rgb_mean                       [[[97.631615 98.70732  98.41285 ]]]\n",
      "rgb_std                        [[[52.766678 52.63513  52.348827]]]\n",
      "root_dir                       /media/gj_data/QAT_Squeezedet/SqueezeDet-PyTorch\n",
      "save_dir                       /media/gj_data/QAT_Squeezedet/SqueezeDet-PyTorch/exp/test_lpd_trt_calib\n",
      "save_intervals                 1\n",
      "score_thresh                   0.5\n",
      "seed                           42\n",
      "stride                         8\n",
      "val_intervals                  1\n",
      "weight_decay                   0.0001\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(cfg.dataset)('val', cfg)\n",
    "cfg = Config().update_dataset_info(cfg, dataset)\n",
    "Config().print(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val\n"
     ]
    }
   ],
   "source": [
    "print (dataset.phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model ../exp/exp14_128_mobv2_stride8/model_best.pth, epoch 410\n",
      "Model successfully loaded.\n",
      "SqueezeDetWithLoss(\n",
      "  (base): SqueezeDetBase(\n",
      "    (quant): QuantStub()\n",
      "    (dequant): DeQuantStub()\n",
      "    (features): Sequential(\n",
      "      (0): ConvBNReLU(\n",
      "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (5): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (7): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (8): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (9): ConvBNReLU(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (convdet): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (resolver): PredictionResolver()\n",
      "  (loss): Loss(\n",
      "    (resolver): PredictionResolver()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if cfg.mode=='eval':\n",
    "        model = SqueezeDetWithLoss(cfg)\n",
    "        model = load_model(model, cfg.load_model, cfg)\n",
    "\n",
    "\n",
    "detect = model.detect\n",
    "model.detect = True\n",
    "detector = Detector(model, cfg)\n",
    "print(detector.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import calib\n",
    "from pytorch_quantization.tensor_quant import QuantDescriptor\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO)  # Disable logging as they are too noisy in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_desc_input = QuantDescriptor(calib_method='histogram')\n",
    "quant_nn.QuantConv2d.set_default_quant_desc_input(quant_desc_input)\n",
    "quant_nn.QuantLinear.set_default_quant_desc_input(quant_desc_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_quantization import quant_modules\n",
    "quant_modules.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_stats(model, data_loader, num_batches):\n",
    "    \"\"\"Feed data to the network and collect statistic\"\"\"\n",
    "\n",
    "    # Enable calibrators\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.disable_quant()\n",
    "                module.enable_calib()\n",
    "            else:\n",
    "                module.disable()\n",
    "\n",
    "    # for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "    #     model(image.cuda())\n",
    "    #     if i >= num_batches:\n",
    "    #         break\n",
    "    # since lpr dataloader return a dict, so writing it in this way\n",
    "    for i, (batch) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "        for k in batch:\n",
    "                if 'image_meta' not in k:\n",
    "                    batch[k] = batch[k].to(device=cfg.device, non_blocking=True)\n",
    "        model(batch)\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "\n",
    "    # Disable calibrators\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.enable_quant()\n",
    "                module.disable_calib()\n",
    "            else:\n",
    "                module.enable()\n",
    "            \n",
    "def compute_amax(model, **kwargs):\n",
    "    # Load calib result\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                    module.load_calib_amax()\n",
    "                else:\n",
    "                    module.load_calib_amax(**kwargs)\n",
    "#             print(F\"{name:40}: {module}\")\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "\n",
    "from datasets.base import BaseDataset\n",
    "from utils.boxes import generate_anchors\n",
    "from PIL import Image\n",
    "from torchvision.datasets.folder import default_loader\n",
    "\n",
    "\n",
    "class LPR_CalibTRT(BaseDataset):\n",
    "    def __init__(self, phase, cfg):\n",
    "        super(LPR_CalibTRT, self).__init__(phase, cfg)\n",
    "\n",
    "        self.input_size = (128, 128)  # (height, width), both dividable by 16\n",
    "        self.class_names = ('0')\n",
    "        # real_filtered mean and std\n",
    "        # self.rgb_mean = np.array([94.87347, 96.89165, 94.70493], dtype=np.float32).reshape(1, 1, 3)\n",
    "        # self.rgb_std = np.array([53.869507, 53.936283, 55.2807], dtype=np.float32).reshape(1, 1, 3)\n",
    "        \n",
    "        # real_filtered plus all_sites_seatbelt mean and std\n",
    "        # self.rgb_mean = np.array([104.90631, 105.41336, 104.70162], dtype=np.float32).reshape(1, 1, 3)\n",
    "        # self.rgb_std = np.array([50.69564, 49.60443, 50.158844], dtype=np.float32).reshape(1, 1, 3)\n",
    "\n",
    "        self.rgb_mean = np.array([97.631615, 98.70732, 98.41285], dtype=np.float32).reshape(1, 1, 3)\n",
    "        self.rgb_std = np.array([52.766678, 52.63513, 52.348827], dtype=np.float32).reshape(1, 1, 3)\n",
    "\n",
    "        self.num_classes = len(self.class_names)\n",
    "        self.class_ids_dict = {cls_name: cls_id for cls_id, cls_name in enumerate(self.class_names)}\n",
    "\n",
    "        self.data_dir = os.path.join(cfg.data_dir, 'lpr_crop/merged_data')\n",
    "        self.sample_ids, self.sample_set_path = self.get_sample_ids()\n",
    "        self.grid_size = tuple(x // cfg.stride for x in self.input_size)  # anchors grid \n",
    "        # self.anchors_seed = np.array([[ 29, 17], [46, 32], [69, 52],\n",
    "        #                                 [109, 68], [84, 127], [155, 106], \n",
    "        #                                 [255, 145], [183, 215], [371, 221]], dtype=np.float32) ## real_filtered anchors\n",
    "        \n",
    "        # self.anchors_seed = np.array( [[ 32, 20], [ 61, 42], [ 59, 97],\n",
    "        #                                 [103, 66], [122, 114], [183, 96],\n",
    "        #                                 [160, 152], [211, 201], [343, 205]], dtype=np.float32) ## real_filtered plus all_sites_seatbelt anchors\n",
    "\n",
    "        # self.anchors_seed = np.array([[6, 5], [12, 10], [18, 10], [18, 18], [20, 24], [30, 15]], dtype=np.float32)\n",
    "        self.anchors_seed = np.array([[3, 3], [6, 5], [9, 5], [10, 9], [10, 12], [15, 8]], dtype=np.float32)\n",
    "        self.anchors = generate_anchors(self.grid_size, self.input_size, self.anchors_seed)\n",
    "        self.anchors_per_grid = self.anchors_seed.shape[0]\n",
    "        self.num_anchors = self.anchors.shape[0]\n",
    "\n",
    "        self.results_dir = os.path.join(cfg.save_dir, 'results')\n",
    "\n",
    "    def get_sample_ids(self):\n",
    "        #a dirty duct tape to load preprocessing of val phase but load dataset for train phase for trt calib\n",
    "        sample_set_name = 'train.txt' if self.phase == 'train' \\\n",
    "            else 'train.txt' if self.phase == 'val' \\\n",
    "            else 'trainval.txt' if self.phase == 'trainval' \\\n",
    "            else None\n",
    "\n",
    "        sample_ids_path = os.path.join(self.data_dir, sample_set_name)\n",
    "        with open(sample_ids_path, 'r') as fp:\n",
    "            sample_ids = fp.readlines()\n",
    "        sample_ids = tuple(x.strip() for x in sample_ids)\n",
    "\n",
    "        return sample_ids, sample_ids_path\n",
    "\n",
    "    def load_image(self, index):\n",
    "        image_id = self.sample_ids[index]\n",
    "        image_path = os.path.join(self.data_dir, 'images', image_id + '.png')\n",
    "        image = default_loader(image_path)\n",
    "        if image.mode == 'L':\n",
    "            image = image.convert('RGB')\n",
    "        image = np.array(image).astype(np.float32)\n",
    "        # image = skimage.io.imread(image_path).astype(np.float32)\n",
    "        return image, image_id\n",
    "\n",
    "    def load_annotations(self, index):\n",
    "        ann_id = self.sample_ids[index]\n",
    "        ann_path = os.path.join(self.data_dir, 'labels', ann_id + '.txt')\n",
    "        with open(ann_path, 'r') as fp:\n",
    "            annotations = fp.readlines()\n",
    "\n",
    "        annotations = [ann.strip().split(' ') for ann in annotations]\n",
    "        class_ids, boxes = [], []\n",
    "        for ann in annotations:\n",
    "            if ann[0] not in self.class_names:\n",
    "                continue\n",
    "            class_ids.append(self.class_ids_dict[ann[0]])\n",
    "            box = [float(x) for x in ann[4:8]]\n",
    "            # if box[2] <= 0:\n",
    "            #     box[2] = 0.00001\n",
    "            # if box[3] <= 0:\n",
    "            #     box[3] = 0.00001\n",
    "            boxes.append(box)\n",
    "\n",
    "        class_ids = np.array(class_ids, dtype=np.int16)\n",
    "        boxes = np.array(boxes, dtype=np.float32)\n",
    "        if len(boxes):\n",
    "            return class_ids, boxes\n",
    "        boxes = None\n",
    "        return class_ids, boxes\n",
    "\n",
    "    # ========================================\n",
    "    #                evaluation\n",
    "    # ========================================\n",
    "\n",
    "    def save_results(self, results):\n",
    "        txt_dir = os.path.join(self.results_dir, 'data')\n",
    "        os.makedirs(txt_dir, exist_ok=True)\n",
    "\n",
    "        for res in results:\n",
    "            txt_path = os.path.join(txt_dir, res['image_meta']['image_id'] + '.txt')\n",
    "            if 'class_ids' not in res:\n",
    "                with open(txt_path, 'w') as fp:\n",
    "                    fp.write('')\n",
    "                continue\n",
    "\n",
    "            num_boxes = len(res['class_ids'])\n",
    "            with open(txt_path, 'w') as fp:\n",
    "                for i in range(num_boxes):\n",
    "                    class_name = self.class_names[res['class_ids'][i]].lower()\n",
    "                    score = res['scores'][i]\n",
    "                    bbox = res['boxes'][i, :]\n",
    "                    line = '{} -1 -1 0 {:.2f} {:.2f} {:.2f} {:.2f} 0 0 0 0 0 0 0 {:.3f}\\n'.format(\n",
    "                            class_name, *bbox, score)\n",
    "                    fp.write(line)\n",
    "\n",
    "    def evaluate(self):\n",
    "        kitti_eval_tool_path = os.path.join(self.cfg.root_dir, 'src/utils/kitti-eval/cpp/evaluate_object')\n",
    "        cmd = '{} {} {} {} {}'.format(kitti_eval_tool_path,\n",
    "                                      os.path.join(self.data_dir),\n",
    "                                      self.sample_set_path,\n",
    "                                      self.results_dir,\n",
    "                                      len(self.sample_ids))\n",
    "\n",
    "        status = subprocess.call(cmd, shell=True)\n",
    "\n",
    "        aps = {}\n",
    "        for class_name in self.class_names:\n",
    "            map_path = os.path.join(self.results_dir, 'stats_{}_ap.txt'.format(class_name.lower()))\n",
    "            if os.path.exists(map_path):\n",
    "                with open(map_path, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                _aps = [float(line.split('=')[1].strip()) for line in lines]\n",
    "            else:\n",
    "                _aps = [0., 0., 0.]\n",
    "\n",
    "            aps[class_name + '_easy'] = _aps[0]\n",
    "            aps[class_name + '_moderate'] = _aps[1]\n",
    "            aps[class_name + '_hard'] = _aps[2]\n",
    "\n",
    "        aps['mAP'] = sum(aps.values()) / len(aps)\n",
    "\n",
    "        return aps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpr_calib_dataset = LPR_CalibTRT('val', cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_data_loader = torch.utils.data.DataLoader(lpr_calib_dataset,\n",
    "                                                  batch_size=cfg.batch_size,\n",
    "                                                  num_workers=cfg.num_workers,\n",
    "                                                  pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (len(calib_data_loader))\n",
    "# inp  = next(iter(calib_data_loader))\n",
    "# print(inp['image'].shape)\n",
    "\n",
    "# for i, inp in enumerate(calib_data_loader, 0):\n",
    "#         print(inp['image'].shape)\n",
    "#         if i >= 10:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0811 22:36:31.199576 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.199994 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.200503 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.200871 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I0811 22:36:31.201338 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.201673 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.202006 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.202298 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I0811 22:36:31.202584 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.202862 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.203136 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.203391 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I0811 22:36:31.203675 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.203927 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.204690 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.204977 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I0811 22:36:31.205252 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.205518 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.205786 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.206049 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I0811 22:36:31.206319 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.206568 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.206824 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.207080 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I0811 22:36:31.207361 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.207683 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.207944 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.208204 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I0811 22:36:31.208484 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.209415 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.209686 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.209959 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I0811 22:36:31.210232 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.210487 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.210757 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.211018 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I0811 22:36:31.211296 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.211550 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.211831 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.212170 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I0811 22:36:31.212469 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.212833 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.213206 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.213566 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I0811 22:36:31.213936 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.214259 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.215329 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.215669 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I0811 22:36:31.216025 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.216310 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.216583 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.216851 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I0811 22:36:31.217129 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.217417 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.217779 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.218142 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I0811 22:36:31.218464 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.218828 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.219120 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.219446 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I0811 22:36:31.219732 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.220710 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.221015 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.221293 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I0811 22:36:31.221572 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.221841 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.222193 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.222522 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I0811 22:36:31.222856 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.223145 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.223407 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.223687 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I0811 22:36:31.223974 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.224231 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.224511 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.224865 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I0811 22:36:31.225163 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.226254 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.226595 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.226892 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I0811 22:36:31.227178 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.227509 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.227943 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.228971 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I0811 22:36:31.229725 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.230194 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.230674 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.231186 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I0811 22:36:31.231544 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.231850 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.232172 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.232523 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I0811 22:36:31.232825 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.233107 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.233717 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.234056 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I0811 22:36:31.234532 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.234830 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.235115 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.235393 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I0811 22:36:31.235672 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.235934 139664402845888 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I0811 22:36:31.236200 139664402845888 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I0811 22:36:31.236483 139664402845888 tensor_quantizer.py:179] Enable MaxCalibrator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:45<00:00,  7.00it/s]\n",
      "I0811 22:41:16.852417 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.852936 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.853235 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.853570 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "I0811 22:41:16.853879 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.854173 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.854442 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.854707 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "I0811 22:41:16.854984 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.855242 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.855503 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.855903 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "I0811 22:41:16.856561 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.856837 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.857107 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.857548 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "I0811 22:41:16.857907 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.858369 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.858740 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.859061 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "I0811 22:41:16.859416 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.859791 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.860110 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.860407 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "I0811 22:41:16.860683 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.860964 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.861283 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.861545 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "I0811 22:41:16.861829 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.862118 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.862417 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.863266 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "I0811 22:41:16.863582 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.863865 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.864128 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.864406 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "I0811 22:41:16.864744 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.865010 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.865275 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.865538 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "I0811 22:41:16.865800 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.866067 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.866324 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.866572 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "I0811 22:41:16.867400 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.867806 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.868174 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.868524 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "I0811 22:41:16.868850 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.869279 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.869573 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.869853 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "I0811 22:41:16.870130 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.870394 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.870638 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.870902 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "I0811 22:41:16.871163 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.871428 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.871728 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.872009 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "I0811 22:41:16.872301 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.872555 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.872809 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.873069 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "I0811 22:41:16.873337 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.873584 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.873845 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.874873 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "I0811 22:41:16.875177 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.875731 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.876520 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.876854 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "I0811 22:41:16.877241 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.877532 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.877802 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.878087 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "I0811 22:41:16.878348 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.878684 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.879344 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.879643 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "I0811 22:41:16.880048 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.880398 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.880737 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.881072 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "I0811 22:41:16.881428 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.881768 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.882055 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.882459 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "I0811 22:41:16.882770 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.883164 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.883435 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.883762 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "I0811 22:41:16.884076 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.884373 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.884635 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.884915 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "I0811 22:41:16.885195 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.885452 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.885706 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.885984 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "I0811 22:41:16.886254 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.886504 139664402845888 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "I0811 22:41:16.886776 139664402845888 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W0811 22:41:16.887057 139664402845888 tensor_quantizer.py:173] Disable MaxCalibrator\n"
     ]
    }
   ],
   "source": [
    "num_batches = int(len(lpr_calib_dataset)/cfg.batch_size)\n",
    "print (num_batches)\n",
    "# lpr_model = detector.model\n",
    "# lpr_model.cuda()\n",
    "# print (lpr_model)\n",
    "detector.model.eval()\n",
    "detector.model.cuda()\n",
    "with torch.no_grad():\n",
    "    collect_stats(detector.model, calib_data_loader, num_batches=2000)\n",
    "    # compute_amax(detector.model, method=\"percentile\", percentile=99.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeDetWithLoss(\n",
      "  (base): SqueezeDetBase(\n",
      "    (quant): QuantStub()\n",
      "    (dequant): DeQuantStub()\n",
      "    (features): Sequential(\n",
      "      (0): ConvBNReLU(\n",
      "        (0): QuantConv2d(\n",
      "          3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "          (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "          (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "        )\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): QuantConv2d(\n",
      "              32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "            )\n",
      "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): QuantConv2d(\n",
      "            32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "          )\n",
      "          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): QuantConv2d(\n",
      "              16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "            )\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): QuantConv2d(\n",
      "              96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False\n",
      "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "            )\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): QuantConv2d(\n",
      "            96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "          )\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): QuantConv2d(\n",
      "              24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "            )\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): QuantConv2d(\n",
      "              144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False\n",
      "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "            )\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): QuantConv2d(\n",
      "            144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "          )\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): QuantConv2d(\n",
      "              24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "            )\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): QuantConv2d(\n",
      "              144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False\n",
      "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "            )\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): QuantConv2d(\n",
      "            144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "          )\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (5): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): QuantConv2d(\n",
      "              32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "            )\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): QuantConv2d(\n",
      "              192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False\n",
      "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "            )\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): QuantConv2d(\n",
      "            192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "          )\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): QuantConv2d(\n",
      "              32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "            )\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): QuantConv2d(\n",
      "              192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False\n",
      "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "            )\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): QuantConv2d(\n",
      "            192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "          )\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (7): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): QuantConv2d(\n",
      "              32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "            )\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): QuantConv2d(\n",
      "              192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False\n",
      "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "            )\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): QuantConv2d(\n",
      "            192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "          )\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (8): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvBNReLU(\n",
      "            (0): QuantConv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "            )\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): ConvBNReLU(\n",
      "            (0): QuantConv2d(\n",
      "              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False\n",
      "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "            )\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): QuantConv2d(\n",
      "            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "          )\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (9): ConvBNReLU(\n",
      "        (0): QuantConv2d(\n",
      "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "          (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "        )\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (convdet): QuantConv2d(\n",
      "      256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "  )\n",
      "  (resolver): PredictionResolver()\n",
      "  (loss): Loss(\n",
      "    (resolver): PredictionResolver()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print (detector.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Different Scehmes and Eval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'val'"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets.lpr import LPR\n",
    "\n",
    "eval_dataset = LPR('val', cfg)\n",
    "eval_dataset.phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0811 22:45:37.243749 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.244675 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([32, 1, 1, 1]).\n",
      "W0811 22:45:37.245321 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.245630 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([32, 1, 1, 1]).\n",
      "W0811 22:45:37.246155 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.246483 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([16, 1, 1, 1]).\n",
      "W0811 22:45:37.247032 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.247324 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([96, 1, 1, 1]).\n",
      "W0811 22:45:37.247841 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.248147 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([96, 1, 1, 1]).\n",
      "W0811 22:45:37.248678 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.249006 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([24, 1, 1, 1]).\n",
      "W0811 22:45:37.249523 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.249834 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([144, 1, 1, 1]).\n",
      "W0811 22:45:37.250365 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.250723 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([144, 1, 1, 1]).\n",
      "W0811 22:45:37.251278 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.251604 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([24, 1, 1, 1]).\n",
      "W0811 22:45:37.252139 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.252435 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([144, 1, 1, 1]).\n",
      "W0811 22:45:37.252928 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.253210 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([144, 1, 1, 1]).\n",
      "W0811 22:45:37.253701 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.253994 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([32, 1, 1, 1]).\n",
      "W0811 22:45:37.254492 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.254786 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([192, 1, 1, 1]).\n",
      "W0811 22:45:37.255279 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.255570 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([192, 1, 1, 1]).\n",
      "W0811 22:45:37.256056 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.256379 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([32, 1, 1, 1]).\n",
      "W0811 22:45:37.256921 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.257241 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([192, 1, 1, 1]).\n",
      "W0811 22:45:37.257626 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.258007 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([192, 1, 1, 1]).\n",
      "W0811 22:45:37.258608 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.258894 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([32, 1, 1, 1]).\n",
      "W0811 22:45:37.259422 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.259746 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([192, 1, 1, 1]).\n",
      "W0811 22:45:37.260304 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.260627 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([192, 1, 1, 1]).\n",
      "W0811 22:45:37.261203 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.261495 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([64, 1, 1, 1]).\n",
      "W0811 22:45:37.262016 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.262311 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([384, 1, 1, 1]).\n",
      "W0811 22:45:37.262790 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.263056 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([384, 1, 1, 1]).\n",
      "W0811 22:45:37.263539 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.263825 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([64, 1, 1, 1]).\n",
      "W0811 22:45:37.264312 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.264595 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([256, 1, 1, 1]).\n",
      "W0811 22:45:37.265067 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:45:37.265352 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([36, 1, 1, 1]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############percentile calibration ######################\n",
      "eval: [0/1530] | data 0.050s | net 0.015s\n",
      "eval: [20/1530] | data 0.004s | net 0.019s\n",
      "eval: [40/1530] | data 0.006s | net 0.021s\n",
      "eval: [60/1530] | data 0.008s | net 0.027s\n",
      "eval: [80/1530] | data 0.014s | net 0.040s\n",
      "eval: [100/1530] | data 0.029s | net 0.075s\n",
      "eval: [120/1530] | data 0.005s | net 0.019s\n",
      "eval: [140/1530] | data 0.031s | net 0.080s\n",
      "eval: [160/1530] | data 0.007s | net 0.022s\n",
      "eval: [180/1530] | data 0.001s | net 0.011s\n",
      "eval: [200/1530] | data 0.014s | net 0.040s\n",
      "eval: [220/1530] | data 0.046s | net 0.133s\n",
      "eval: [240/1530] | data 0.016s | net 0.046s\n",
      "eval: [260/1530] | data 0.005s | net 0.019s\n",
      "eval: [280/1530] | data 0.001s | net 0.011s\n",
      "eval: [300/1530] | data 0.011s | net 0.034s\n",
      "eval: [320/1530] | data 0.004s | net 0.018s\n",
      "eval: [340/1530] | data 0.011s | net 0.033s\n",
      "eval: [360/1530] | data 0.022s | net 0.069s\n",
      "eval: [380/1530] | data 0.008s | net 0.026s\n",
      "eval: [400/1530] | data 0.022s | net 0.057s\n",
      "eval: [420/1530] | data 0.009s | net 0.015s\n",
      "eval: [440/1530] | data 0.036s | net 0.095s\n",
      "eval: [460/1530] | data 0.002s | net 0.012s\n",
      "eval: [480/1530] | data 0.001s | net 0.012s\n",
      "eval: [500/1530] | data 0.008s | net 0.025s\n",
      "eval: [520/1530] | data 0.005s | net 0.020s\n",
      "eval: [540/1530] | data 0.002s | net 0.015s\n",
      "eval: [560/1530] | data 0.015s | net 0.043s\n",
      "eval: [580/1530] | data 0.032s | net 0.082s\n",
      "eval: [600/1530] | data 0.009s | net 0.030s\n",
      "eval: [620/1530] | data 0.002s | net 0.012s\n",
      "eval: [640/1530] | data 0.003s | net 0.016s\n",
      "eval: [660/1530] | data 0.002s | net 0.012s\n",
      "eval: [680/1530] | data 0.006s | net 0.020s\n",
      "eval: [700/1530] | data 0.007s | net 0.025s\n",
      "eval: [720/1530] | data 0.004s | net 0.017s\n",
      "eval: [740/1530] | data 0.006s | net 0.011s\n",
      "eval: [760/1530] | data 0.006s | net 0.023s\n",
      "eval: [780/1530] | data 0.005s | net 0.021s\n",
      "eval: [800/1530] | data 0.004s | net 0.016s\n",
      "eval: [820/1530] | data 0.012s | net 0.041s\n",
      "eval: [840/1530] | data 0.008s | net 0.028s\n",
      "eval: [860/1530] | data 0.015s | net 0.046s\n",
      "eval: [880/1530] | data 0.003s | net 0.016s\n",
      "eval: [900/1530] | data 0.001s | net 0.011s\n",
      "eval: [920/1530] | data 0.006s | net 0.022s\n",
      "eval: [940/1530] | data 0.002s | net 0.013s\n",
      "eval: [960/1530] | data 0.017s | net 0.034s\n",
      "eval: [980/1530] | data 0.021s | net 0.058s\n",
      "eval: [1000/1530] | data 0.019s | net 0.040s\n",
      "eval: [1020/1530] | data 0.030s | net 0.078s\n",
      "eval: [1040/1530] | data 0.023s | net 0.062s\n",
      "eval: [1060/1530] | data 0.002s | net 0.014s\n",
      "eval: [1080/1530] | data 0.002s | net 0.012s\n",
      "eval: [1100/1530] | data 0.018s | net 0.010s\n",
      "eval: [1120/1530] | data 0.017s | net 0.048s\n",
      "eval: [1140/1530] | data 0.008s | net 0.026s\n",
      "eval: [1160/1530] | data 0.004s | net 0.018s\n",
      "eval: [1180/1530] | data 0.008s | net 0.026s\n",
      "eval: [1200/1530] | data 0.008s | net 0.029s\n",
      "eval: [1220/1530] | data 0.024s | net 0.066s\n",
      "eval: [1240/1530] | data 0.005s | net 0.010s\n",
      "eval: [1260/1530] | data 0.002s | net 0.012s\n",
      "eval: [1280/1530] | data 0.003s | net 0.014s\n",
      "eval: [1300/1530] | data 0.018s | net 0.052s\n",
      "eval: [1320/1530] | data 0.009s | net 0.028s\n",
      "eval: [1340/1530] | data 0.031s | net 0.082s\n",
      "eval: [1360/1530] | data 0.008s | net 0.026s\n",
      "eval: [1380/1530] | data 0.001s | net 0.009s\n",
      "eval: [1400/1530] | data 0.002s | net 0.013s\n",
      "eval: [1420/1530] | data 0.031s | net 0.080s\n",
      "eval: [1440/1530] | data 0.002s | net 0.015s\n",
      "eval: [1460/1530] | data 0.003s | net 0.017s\n",
      "eval: [1480/1530] | data 0.011s | net 0.033s\n",
      "eval: [1500/1530] | data 0.006s | net 0.023s\n",
      "eval: [1520/1530] | data 0.003s | net 0.014s\n",
      "Elapsed 1.03min (40.3ms/image, 24.8frames/s)\n",
      "--------------------------------------------------------------------------------\n",
      "gt dir:/home/gj/hazen/QAT_Squeezedet/SqueezeDet-PyTorch/data/lpr_crop/merged_data/labels\n",
      "Thank you for participating in our evaluation!\n",
      "filename: /home/gj/hazen/QAT_Squeezedet/SqueezeDet-PyTorch/data/lpr_crop/merged_data/val.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: 1: gnuplot: not found\n",
      "sh: 1: gnuplot: not found\n",
      "Error: /undefinedfilename in (car_detection.eps)\n",
      "Operand stack:\n",
      "\n",
      "Execution stack:\n",
      "   %interp_exit   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--   --nostringval--   --nostringval--   false   1   %stopped_push\n",
      "Dictionary stack:\n",
      "   --dict:956/1684(ro)(G)--   --dict:0/20(G)--   --dict:77/200(L)--\n",
      "Current allocation mode is local\n",
      "Last OS error: No such file or directory\n",
      "GPL Ghostscript 9.26: Unrecoverable error, exit code 1\n",
      "sh: 1: pdfcrop: not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading detections...\n",
      "  done.\n",
      "Your evaluation results are available in /home/gj/hazen/QAT_Squeezedet/SqueezeDet-PyTorch/exp/test_lpd_trt_calib/results\n",
      "0_easy               0.892\n",
      "0_moderate           0.892\n",
      "0_hard               0.892\n",
      "mAP                  0.892\n",
      " ############mse calibration ######################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0811 22:46:40.742229 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:46:40.743023 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([32, 1, 1, 1]).\n",
      "W0811 22:46:43.376795 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:46:43.377391 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([32, 1, 1, 1]).\n",
      "W0811 22:46:44.646888 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:46:44.647568 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([16, 1, 1, 1]).\n",
      "W0811 22:46:47.974412 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:46:47.975063 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([96, 1, 1, 1]).\n",
      "W0811 22:46:49.985022 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:46:49.985890 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([96, 1, 1, 1]).\n",
      "W0811 22:46:54.274832 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:46:54.275444 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([24, 1, 1, 1]).\n",
      "W0811 22:46:57.974802 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:46:57.975459 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([144, 1, 1, 1]).\n",
      "W0811 22:47:01.169631 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:47:01.170250 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([144, 1, 1, 1]).\n",
      "W0811 22:47:03.277160 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:47:03.277740 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([24, 1, 1, 1]).\n",
      "W0811 22:47:05.538303 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:47:05.538934 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([144, 1, 1, 1]).\n",
      "W0811 22:47:07.261236 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:47:07.262645 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([144, 1, 1, 1]).\n",
      "W0811 22:47:08.816451 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:47:08.817145 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([32, 1, 1, 1]).\n",
      "W0811 22:47:10.081097 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:47:10.081775 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([192, 1, 1, 1]).\n",
      "W0811 22:47:11.026111 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:47:11.026896 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([192, 1, 1, 1]).\n",
      "W0811 22:47:12.226397 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:47:12.227137 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([32, 1, 1, 1]).\n",
      "W0811 22:47:13.510152 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:47:13.510891 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([192, 1, 1, 1]).\n",
      "W0811 22:47:14.557480 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:47:14.558209 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([192, 1, 1, 1]).\n",
      "W0811 22:47:15.486113 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:47:15.486841 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([32, 1, 1, 1]).\n",
      "W0811 22:47:16.761288 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:47:16.762018 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([192, 1, 1, 1]).\n",
      "W0811 22:47:17.846250 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:47:17.846984 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([192, 1, 1, 1]).\n",
      "W0811 22:47:19.344157 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:47:19.344890 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([64, 1, 1, 1]).\n",
      "W0811 22:47:20.250181 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:47:20.250897 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([384, 1, 1, 1]).\n",
      "W0811 22:47:21.047879 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:47:21.048600 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([384, 1, 1, 1]).\n",
      "W0811 22:47:22.079465 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:47:22.080201 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([64, 1, 1, 1]).\n",
      "W0811 22:47:22.997853 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:47:22.998580 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([256, 1, 1, 1]).\n",
      "W0811 22:47:23.926517 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:47:23.927206 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([36, 1, 1, 1]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval: [0/1530] | data 0.003s | net 0.013s\n",
      "eval: [20/1530] | data 0.004s | net 0.019s\n",
      "eval: [40/1530] | data 0.006s | net 0.022s\n",
      "eval: [60/1530] | data 0.009s | net 0.027s\n",
      "eval: [80/1530] | data 0.014s | net 0.040s\n",
      "eval: [100/1530] | data 0.029s | net 0.075s\n",
      "eval: [120/1530] | data 0.004s | net 0.020s\n",
      "eval: [140/1530] | data 0.031s | net 0.079s\n",
      "eval: [160/1530] | data 0.006s | net 0.022s\n",
      "eval: [180/1530] | data 0.001s | net 0.012s\n",
      "eval: [200/1530] | data 0.013s | net 0.040s\n",
      "eval: [220/1530] | data 0.046s | net 0.134s\n",
      "eval: [240/1530] | data 0.016s | net 0.046s\n",
      "eval: [260/1530] | data 0.005s | net 0.020s\n",
      "eval: [280/1530] | data 0.001s | net 0.011s\n",
      "eval: [300/1530] | data 0.011s | net 0.034s\n",
      "eval: [320/1530] | data 0.004s | net 0.018s\n",
      "eval: [340/1530] | data 0.011s | net 0.033s\n",
      "eval: [360/1530] | data 0.019s | net 0.058s\n",
      "eval: [380/1530] | data 0.008s | net 0.027s\n",
      "eval: [400/1530] | data 0.021s | net 0.056s\n",
      "eval: [420/1530] | data 0.003s | net 0.015s\n",
      "eval: [440/1530] | data 0.036s | net 0.095s\n",
      "eval: [460/1530] | data 0.002s | net 0.013s\n",
      "eval: [480/1530] | data 0.001s | net 0.011s\n",
      "eval: [500/1530] | data 0.007s | net 0.025s\n",
      "eval: [520/1530] | data 0.005s | net 0.020s\n",
      "eval: [540/1530] | data 0.003s | net 0.014s\n",
      "eval: [560/1530] | data 0.015s | net 0.043s\n",
      "eval: [580/1530] | data 0.032s | net 0.082s\n",
      "eval: [600/1530] | data 0.009s | net 0.030s\n",
      "eval: [620/1530] | data 0.002s | net 0.012s\n",
      "eval: [640/1530] | data 0.003s | net 0.016s\n",
      "eval: [660/1530] | data 0.002s | net 0.013s\n",
      "eval: [680/1530] | data 0.005s | net 0.020s\n",
      "eval: [700/1530] | data 0.007s | net 0.026s\n",
      "eval: [720/1530] | data 0.004s | net 0.017s\n",
      "eval: [740/1530] | data 0.006s | net 0.010s\n",
      "eval: [760/1530] | data 0.006s | net 0.023s\n",
      "eval: [780/1530] | data 0.005s | net 0.022s\n",
      "eval: [800/1530] | data 0.004s | net 0.016s\n",
      "eval: [820/1530] | data 0.012s | net 0.041s\n",
      "eval: [840/1530] | data 0.008s | net 0.029s\n",
      "eval: [860/1530] | data 0.019s | net 0.052s\n",
      "eval: [880/1530] | data 0.003s | net 0.016s\n",
      "eval: [900/1530] | data 0.001s | net 0.012s\n",
      "eval: [920/1530] | data 0.006s | net 0.022s\n",
      "eval: [940/1530] | data 0.002s | net 0.013s\n",
      "eval: [960/1530] | data 0.012s | net 0.034s\n",
      "eval: [980/1530] | data 0.021s | net 0.057s\n",
      "eval: [1000/1530] | data 0.014s | net 0.041s\n",
      "eval: [1020/1530] | data 0.030s | net 0.079s\n",
      "eval: [1040/1530] | data 0.023s | net 0.061s\n",
      "eval: [1060/1530] | data 0.003s | net 0.013s\n",
      "eval: [1080/1530] | data 0.002s | net 0.012s\n",
      "eval: [1100/1530] | data 0.018s | net 0.010s\n",
      "eval: [1120/1530] | data 0.017s | net 0.047s\n",
      "eval: [1140/1530] | data 0.007s | net 0.024s\n",
      "eval: [1160/1530] | data 0.004s | net 0.018s\n",
      "eval: [1180/1530] | data 0.008s | net 0.027s\n",
      "eval: [1200/1530] | data 0.008s | net 0.029s\n",
      "eval: [1220/1530] | data 0.025s | net 0.065s\n",
      "eval: [1240/1530] | data 0.004s | net 0.010s\n",
      "eval: [1260/1530] | data 0.002s | net 0.012s\n",
      "eval: [1280/1530] | data 0.002s | net 0.013s\n",
      "eval: [1300/1530] | data 0.018s | net 0.051s\n",
      "eval: [1320/1530] | data 0.009s | net 0.028s\n",
      "eval: [1340/1530] | data 0.031s | net 0.082s\n",
      "eval: [1360/1530] | data 0.008s | net 0.026s\n",
      "eval: [1380/1530] | data 0.001s | net 0.011s\n",
      "eval: [1400/1530] | data 0.002s | net 0.012s\n",
      "eval: [1420/1530] | data 0.031s | net 0.081s\n",
      "eval: [1440/1530] | data 0.003s | net 0.015s\n",
      "eval: [1460/1530] | data 0.034s | net 0.017s\n",
      "eval: [1480/1530] | data 0.012s | net 0.033s\n",
      "eval: [1500/1530] | data 0.009s | net 0.022s\n",
      "eval: [1520/1530] | data 0.003s | net 0.015s\n",
      "Elapsed 1.01min (39.8ms/image, 25.2frames/s)\n",
      "--------------------------------------------------------------------------------\n",
      "gt dir:/home/gj/hazen/QAT_Squeezedet/SqueezeDet-PyTorch/data/lpr_crop/merged_data/labels\n",
      "Thank you for participating in our evaluation!\n",
      "filename: /home/gj/hazen/QAT_Squeezedet/SqueezeDet-PyTorch/data/lpr_crop/merged_data/val.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/gj/hazen/QAT_Squeezedet/SqueezeDet-PyTorch/exp/test_lpd_trt_calib/results/plot’: File exists\n",
      "sh: 1: gnuplot: not found\n",
      "sh: 1: gnuplot: not found\n",
      "Error: /undefinedfilename in (car_detection.eps)\n",
      "Operand stack:\n",
      "\n",
      "Execution stack:\n",
      "   %interp_exit   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--   --nostringval--   --nostringval--   false   1   %stopped_push\n",
      "Dictionary stack:\n",
      "   --dict:956/1684(ro)(G)--   --dict:0/20(G)--   --dict:77/200(L)--\n",
      "Current allocation mode is local\n",
      "Last OS error: No such file or directory\n",
      "GPL Ghostscript 9.26: Unrecoverable error, exit code 1\n",
      "sh: 1: pdfcrop: not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading detections...\n",
      "  done.\n",
      "Your evaluation results are available in /home/gj/hazen/QAT_Squeezedet/SqueezeDet-PyTorch/exp/test_lpd_trt_calib/results\n",
      "0_easy               0.893\n",
      "0_moderate           0.893\n",
      "0_hard               0.893\n",
      "mAP                  0.893\n",
      " ############entropy calibration ######################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0811 22:48:31.195484 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:48:31.196293 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([32, 1, 1, 1]).\n",
      "W0811 22:49:07.994331 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:49:08.011952 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([32, 1, 1, 1]).\n",
      "W0811 22:49:17.569306 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:49:17.569837 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([16, 1, 1, 1]).\n",
      "W0811 22:50:02.644994 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:50:02.651050 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([96, 1, 1, 1]).\n",
      "W0811 22:50:20.069070 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:50:20.072360 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([96, 1, 1, 1]).\n",
      "W0811 22:51:35.103475 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:51:35.119583 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([24, 1, 1, 1]).\n",
      "W0811 22:52:10.001210 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:52:10.016854 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([144, 1, 1, 1]).\n",
      "W0811 22:53:00.539143 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:53:00.548750 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([144, 1, 1, 1]).\n",
      "W0811 22:53:27.428283 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:53:27.432178 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([24, 1, 1, 1]).\n",
      "W0811 22:53:50.637869 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:53:50.651431 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([144, 1, 1, 1]).\n",
      "W0811 22:54:05.014405 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:54:05.015027 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([144, 1, 1, 1]).\n",
      "W0811 22:54:20.814321 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:54:20.815902 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([32, 1, 1, 1]).\n",
      "W0811 22:54:30.442674 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:54:30.443331 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([192, 1, 1, 1]).\n",
      "W0811 22:54:36.644165 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:54:36.644739 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([192, 1, 1, 1]).\n",
      "W0811 22:54:46.290908 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:54:46.291583 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([32, 1, 1, 1]).\n",
      "W0811 22:54:57.372687 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:54:57.373305 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([192, 1, 1, 1]).\n",
      "W0811 22:55:05.221981 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:55:05.222608 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([192, 1, 1, 1]).\n",
      "W0811 22:55:11.478400 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:55:11.479028 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([32, 1, 1, 1]).\n",
      "W0811 22:55:23.153659 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:55:23.154288 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([192, 1, 1, 1]).\n",
      "W0811 22:55:31.857755 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:55:31.858387 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([192, 1, 1, 1]).\n",
      "W0811 22:55:45.156605 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:55:45.157235 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([64, 1, 1, 1]).\n",
      "W0811 22:55:51.589625 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:55:51.590273 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([384, 1, 1, 1]).\n",
      "W0811 22:55:56.323090 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:55:56.323706 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([384, 1, 1, 1]).\n",
      "W0811 22:56:03.531403 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:56:03.532043 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([64, 1, 1, 1]).\n",
      "W0811 22:56:09.584602 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:56:09.585311 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([256, 1, 1, 1]).\n",
      "W0811 22:56:15.320659 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0811 22:56:15.321280 139664402845888 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([36, 1, 1, 1]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval: [0/1530] | data 0.003s | net 0.013s\n",
      "eval: [20/1530] | data 0.004s | net 0.018s\n",
      "eval: [40/1530] | data 0.006s | net 0.022s\n",
      "eval: [60/1530] | data 0.008s | net 0.028s\n",
      "eval: [80/1530] | data 0.014s | net 0.040s\n",
      "eval: [100/1530] | data 0.029s | net 0.075s\n",
      "eval: [120/1530] | data 0.004s | net 0.019s\n",
      "eval: [140/1530] | data 0.031s | net 0.079s\n",
      "eval: [160/1530] | data 0.007s | net 0.022s\n",
      "eval: [180/1530] | data 0.011s | net 0.012s\n",
      "eval: [200/1530] | data 0.013s | net 0.040s\n",
      "eval: [220/1530] | data 0.046s | net 0.135s\n",
      "eval: [240/1530] | data 0.017s | net 0.048s\n",
      "eval: [260/1530] | data 0.015s | net 0.021s\n",
      "eval: [280/1530] | data 0.001s | net 0.012s\n",
      "eval: [300/1530] | data 0.014s | net 0.034s\n",
      "eval: [320/1530] | data 0.004s | net 0.018s\n",
      "eval: [340/1530] | data 0.014s | net 0.034s\n",
      "eval: [360/1530] | data 0.022s | net 0.068s\n",
      "eval: [380/1530] | data 0.008s | net 0.026s\n",
      "eval: [400/1530] | data 0.023s | net 0.056s\n",
      "eval: [420/1530] | data 0.003s | net 0.015s\n",
      "eval: [440/1530] | data 0.037s | net 0.096s\n",
      "eval: [460/1530] | data 0.004s | net 0.012s\n",
      "eval: [480/1530] | data 0.001s | net 0.011s\n",
      "eval: [500/1530] | data 0.007s | net 0.024s\n",
      "eval: [520/1530] | data 0.005s | net 0.020s\n",
      "eval: [540/1530] | data 0.002s | net 0.015s\n",
      "eval: [560/1530] | data 0.016s | net 0.043s\n",
      "eval: [580/1530] | data 0.092s | net 0.083s\n",
      "eval: [600/1530] | data 0.009s | net 0.030s\n",
      "eval: [620/1530] | data 0.002s | net 0.013s\n",
      "eval: [640/1530] | data 0.003s | net 0.016s\n",
      "eval: [660/1530] | data 0.002s | net 0.013s\n",
      "eval: [680/1530] | data 0.005s | net 0.020s\n",
      "eval: [700/1530] | data 0.008s | net 0.026s\n",
      "eval: [720/1530] | data 0.004s | net 0.018s\n",
      "eval: [740/1530] | data 0.007s | net 0.011s\n",
      "eval: [760/1530] | data 0.006s | net 0.023s\n",
      "eval: [780/1530] | data 0.005s | net 0.021s\n",
      "eval: [800/1530] | data 0.004s | net 0.018s\n",
      "eval: [820/1530] | data 0.012s | net 0.041s\n",
      "eval: [840/1530] | data 0.008s | net 0.030s\n",
      "eval: [860/1530] | data 0.022s | net 0.053s\n",
      "eval: [880/1530] | data 0.003s | net 0.017s\n",
      "eval: [900/1530] | data 0.002s | net 0.012s\n",
      "eval: [920/1530] | data 0.025s | net 0.022s\n",
      "eval: [940/1530] | data 0.004s | net 0.015s\n",
      "eval: [960/1530] | data 0.012s | net 0.036s\n",
      "eval: [980/1530] | data 0.021s | net 0.058s\n",
      "eval: [1000/1530] | data 0.015s | net 0.042s\n",
      "eval: [1020/1530] | data 0.030s | net 0.080s\n",
      "eval: [1040/1530] | data 0.023s | net 0.063s\n",
      "eval: [1060/1530] | data 0.002s | net 0.015s\n",
      "eval: [1080/1530] | data 0.008s | net 0.013s\n",
      "eval: [1100/1530] | data 0.018s | net 0.011s\n",
      "eval: [1120/1530] | data 0.018s | net 0.049s\n",
      "eval: [1140/1530] | data 0.009s | net 0.027s\n",
      "eval: [1160/1530] | data 0.007s | net 0.018s\n",
      "eval: [1180/1530] | data 0.009s | net 0.027s\n",
      "eval: [1200/1530] | data 0.008s | net 0.031s\n",
      "eval: [1220/1530] | data 0.053s | net 0.067s\n",
      "eval: [1240/1530] | data 0.005s | net 0.010s\n",
      "eval: [1260/1530] | data 0.027s | net 0.013s\n",
      "eval: [1280/1530] | data 0.002s | net 0.014s\n",
      "eval: [1300/1530] | data 0.019s | net 0.052s\n",
      "eval: [1320/1530] | data 0.009s | net 0.029s\n",
      "eval: [1340/1530] | data 0.034s | net 0.084s\n",
      "eval: [1360/1530] | data 0.006s | net 0.025s\n",
      "eval: [1380/1530] | data 0.009s | net 0.011s\n",
      "eval: [1400/1530] | data 0.002s | net 0.014s\n",
      "eval: [1420/1530] | data 0.031s | net 0.082s\n",
      "eval: [1440/1530] | data 0.003s | net 0.016s\n",
      "eval: [1460/1530] | data 0.003s | net 0.017s\n",
      "eval: [1480/1530] | data 0.029s | net 0.036s\n",
      "eval: [1500/1530] | data 0.007s | net 0.023s\n",
      "eval: [1520/1530] | data 0.003s | net 0.015s\n",
      "Elapsed 1.09min (42.6ms/image, 23.5frames/s)\n",
      "--------------------------------------------------------------------------------\n",
      "gt dir:/home/gj/hazen/QAT_Squeezedet/SqueezeDet-PyTorch/data/lpr_crop/merged_data/labels\n",
      "Thank you for participating in our evaluation!\n",
      "filename: /home/gj/hazen/QAT_Squeezedet/SqueezeDet-PyTorch/data/lpr_crop/merged_data/val.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/gj/hazen/QAT_Squeezedet/SqueezeDet-PyTorch/exp/test_lpd_trt_calib/results/plot’: File exists\n",
      "sh: 1: gnuplot: not found\n",
      "sh: 1: gnuplot: not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading detections...\n",
      "  done.\n",
      "Your evaluation results are available in /home/gj/hazen/QAT_Squeezedet/SqueezeDet-PyTorch/exp/test_lpd_trt_calib/results\n",
      "0_easy               0.891\n",
      "0_moderate           0.891\n",
      "0_hard               0.891\n",
      "mAP                  0.891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: /undefinedfilename in (car_detection.eps)\n",
      "Operand stack:\n",
      "\n",
      "Execution stack:\n",
      "   %interp_exit   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--   --nostringval--   --nostringval--   false   1   %stopped_push\n",
      "Dictionary stack:\n",
      "   --dict:956/1684(ro)(G)--   --dict:0/20(G)--   --dict:77/200(L)--\n",
      "Current allocation mode is local\n",
      "Last OS error: No such file or directory\n",
      "GPL Ghostscript 9.26: Unrecoverable error, exit code 1\n",
      "sh: 1: pdfcrop: not found\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for method in [\"percentile\", \"mse\", \"entropy\"]:\n",
    "        percentile = 99.99\n",
    "        print(F\" ############  {method} calibration ######################\")\n",
    "        if(method == 'percentile'):\n",
    "            compute_amax(detector.model, method=method, percentile = percentile)\n",
    "        else:\n",
    "            compute_amax(detector.model, method=method)\n",
    "        update_exp_dir(cfg, f\"trt_calib_lpd_{method}\")\n",
    "        results = detector.detect_dataset(eval_dataset, cfg)\n",
    "        eval_dataset.save_results(results)\n",
    "        aps = eval_dataset.evaluate()\n",
    "        for k, v in aps.items():\n",
    "                print('{:<20} {:.3f}'.format(k, v))\n",
    "        torch.save(detector.model.state_dict(), os.path.join(cfg.save_dir, f\"quant_lp_squeezedet_{method}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'val'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print (detector.model)\n",
    "# print (detector.model.base.features[0][0].input_quantizer.amax)\n",
    "# print (detector.model.base.features[0][0].weight_quantizer.amax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_dataset.save_results(results)\n",
    "# aps = eval_dataset.evaluate()\n",
    "# for k, v in aps.items():\n",
    "#         print('{:<20} {:.3f}'.format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(detector.model.state_dict(), \"../exp/quant_lp_squeezedet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = detector.model.base\n",
    "# test.cpu()\n",
    "# quant_nn.TensorQuantizer.use_fb_fake_quant = True\n",
    "# dummy_input = torch.randn(1, 3, 128, 128)\n",
    "\n",
    "# # input_names = [ \"network.0\" ]\n",
    "# # output_names = [ \"network.23\" ]\n",
    "\n",
    "# # enable_onnx_checker needs to be disabled. See notes below.\n",
    "# torch.onnx.export(\n",
    "#     test, dummy_input, \"../exp/test.onnx\", verbose=True, opset_version=13, enable_onnx_checker=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('squeezedet_qat_trt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3c2669bd7e07172b05d24bdf6d2bf9c1c04a6520ab7af118b1064f77a2781ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
