{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_quantization import tensor_quant\n",
    "\n",
    "from torch import nn, outer\n",
    "\n",
    "from pytorch_quantization import tensor_quant\n",
    "import pytorch_quantization.nn as quant_nn\n",
    "import pytorch_quantization\n",
    "pytorch_quantization.__version__\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.yolo import YOLO\n",
    "from engine.detector import Detector\n",
    "from model.squeezedet import SqueezeDet, SqueezeDetWithLoss\n",
    "from utils.config import Config\n",
    "from utils.model import load_model\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "from utils.misc import init_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arch                           squeezedet\n",
      "batch_size                     1\n",
      "bbox_loss_weight               20.0\n",
      "chunk_sizes                    [32]\n",
      "class_loss_weight              1.0\n",
      "data_dir                       /home/hazen/data/datasets\n",
      "dataset                        lpr\n",
      "debug                          2\n",
      "debug_dir                      /home/hazen/data/SqueezeDet-PyTorch/exp/test_lpd_sqdet_trt_calib/debug\n",
      "device                         cpu\n",
      "drift_prob                     1.0\n",
      "dropout_prob                   0\n",
      "exp_dir                        /home/hazen/data/SqueezeDet-PyTorch/exp\n",
      "exp_id                         test_lpd_sqdet_trt_calib\n",
      "flip_prob                      0.5\n",
      "forbid_resize                  False\n",
      "gpus                           [0]\n",
      "gpus_str                       0\n",
      "grad_norm                      0.5\n",
      "keep_top_k                     64\n",
      "load_model                     ../exp/alpr_det.pth\n",
      "load_pretrained                True\n",
      "log_file                       /home/hazen/data/SqueezeDet-PyTorch/exp/test_lpd_sqdet_trt_calib/training_logs.txt\n",
      "lr                             0.001\n",
      "master_batch_size              32\n",
      "mode                           eval\n",
      "momentum                       0.9\n",
      "negative_score_loss_weight     100.0\n",
      "nms_thresh                     0.4\n",
      "no_eval                        False\n",
      "not_cuda_benchmark             False\n",
      "num_epochs                     100\n",
      "num_iters                      -1\n",
      "num_workers                    0\n",
      "positive_score_loss_weight     3.75\n",
      "print_interval                 20\n",
      "qat                            False\n",
      "root_dir                       /home/hazen/data/SqueezeDet-PyTorch\n",
      "save_dir                       /home/hazen/data/SqueezeDet-PyTorch/exp/test_lpd_sqdet_trt_calib\n",
      "save_intervals                 1\n",
      "score_thresh                   0.5\n",
      "seed                           42\n",
      "stride                         16\n",
      "val_intervals                  1\n",
      "weight_decay                   0.0001\n"
     ]
    }
   ],
   "source": [
    "args = {'_': '_'}\n",
    "cfg = Config().parse(args)\n",
    "\n",
    "init_env(cfg) \n",
    " \n",
    "cfg.mode = 'eval'\n",
    "cfg.exp_id = \"test_lpd_sqdet_trt_calib\"\n",
    "cfg.dataset = 'lpr'\n",
    "cfg.load_model = '../exp/alpr_det.pth' \n",
    "cfg.batch_size = 1\n",
    "cfg.arch = 'squeezedet'\n",
    "cfg.num_workers = 0\n",
    "cfg.save_intervals = 1\n",
    "cfg.debug = 2\n",
    "cfg.load_pretrained = True\n",
    "\n",
    "def update_exp_dir(cfg, exp_id):\n",
    "    cfg.save_dir = os.path.join(cfg.exp_dir, exp_id)\n",
    "    os.makedirs(cfg.save_dir, exist_ok=True)\n",
    "    cfg.log_file = os.path.join(cfg.save_dir, 'training_logs.txt')\n",
    "    os.remove(cfg.log_file) if os.path.exists(cfg.log_file) else None\n",
    "    cfg.debug_dir = os.path.join(cfg.save_dir, 'debug')\n",
    "\n",
    "update_exp_dir(cfg, cfg.exp_id)\n",
    "\n",
    "Config().print(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine.detector import Detector\n",
    "from model.squeezedet import SqueezeDetWithLoss\n",
    "from utils.model import load_model\n",
    "from utils.misc import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchors                        [[  8.   8.   3.   3.]\n",
      " [  8.   8.   6.   5.]\n",
      " [  8.   8.   9.   5.]\n",
      " ...\n",
      " [120. 120.  10.   9.]\n",
      " [120. 120.  10.  12.]\n",
      " [120. 120.  15.   8.]]\n",
      "anchors_per_grid               6\n",
      "arch                           squeezedet\n",
      "batch_size                     1\n",
      "bbox_loss_weight               20.0\n",
      "chunk_sizes                    [32]\n",
      "class_loss_weight              1.0\n",
      "class_names                    0\n",
      "data_dir                       /home/hazen/data/datasets\n",
      "dataset                        lpr\n",
      "debug                          2\n",
      "debug_dir                      /home/hazen/data/SqueezeDet-PyTorch/exp/test_lpd_sqdet_trt_calib/debug\n",
      "device                         cpu\n",
      "drift_prob                     1.0\n",
      "dropout_prob                   0\n",
      "exp_dir                        /home/hazen/data/SqueezeDet-PyTorch/exp\n",
      "exp_id                         test_lpd_sqdet_trt_calib\n",
      "flip_prob                      0.5\n",
      "forbid_resize                  False\n",
      "gpus                           [0]\n",
      "gpus_str                       0\n",
      "grad_norm                      0.5\n",
      "grid_size                      (8, 8)\n",
      "input_size                     (128, 128)\n",
      "keep_top_k                     64\n",
      "load_model                     ../exp/alpr_det.pth\n",
      "load_pretrained                True\n",
      "log_file                       /home/hazen/data/SqueezeDet-PyTorch/exp/test_lpd_sqdet_trt_calib/training_logs.txt\n",
      "lr                             0.001\n",
      "master_batch_size              32\n",
      "mode                           eval\n",
      "momentum                       0.9\n",
      "negative_score_loss_weight     100.0\n",
      "nms_thresh                     0.4\n",
      "no_eval                        False\n",
      "not_cuda_benchmark             False\n",
      "num_anchors                    384\n",
      "num_classes                    1\n",
      "num_epochs                     100\n",
      "num_iters                      -1\n",
      "num_workers                    0\n",
      "positive_score_loss_weight     3.75\n",
      "print_interval                 20\n",
      "qat                            False\n",
      "rgb_mean                       [[[97.631615 98.70732  98.41285 ]]]\n",
      "rgb_std                        [[[52.766678 52.63513  52.348827]]]\n",
      "root_dir                       /home/hazen/data/SqueezeDet-PyTorch\n",
      "save_dir                       /home/hazen/data/SqueezeDet-PyTorch/exp/test_lpd_sqdet_trt_calib\n",
      "save_intervals                 1\n",
      "score_thresh                   0.5\n",
      "seed                           42\n",
      "stride                         16\n",
      "val_intervals                  1\n",
      "weight_decay                   0.0001\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(cfg.dataset)('val', cfg)\n",
    "cfg = Config().update_dataset_info(cfg, dataset)\n",
    "Config().print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchors                        [[  8.   8.   3.   3.]\n",
      " [  8.   8.   6.   5.]\n",
      " [  8.   8.   9.   5.]\n",
      " ...\n",
      " [120. 120.  10.   9.]\n",
      " [120. 120.  10.  12.]\n",
      " [120. 120.  15.   8.]]\n",
      "anchors_per_grid               6\n",
      "arch                           squeezedet\n",
      "batch_size                     1\n",
      "bbox_loss_weight               20.0\n",
      "chunk_sizes                    [32]\n",
      "class_loss_weight              1.0\n",
      "class_names                    0\n",
      "data_dir                       /home/hazen/data/datasets\n",
      "dataset                        lpr\n",
      "debug                          2\n",
      "debug_dir                      /home/hazen/data/SqueezeDet-PyTorch/exp/test_lpd_sqdet_trt_calib/debug\n",
      "device                         cpu\n",
      "drift_prob                     1.0\n",
      "dropout_prob                   0\n",
      "exp_dir                        /home/hazen/data/SqueezeDet-PyTorch/exp\n",
      "exp_id                         test_lpd_sqdet_trt_calib\n",
      "flip_prob                      0.5\n",
      "forbid_resize                  False\n",
      "gpus                           [0]\n",
      "gpus_str                       0\n",
      "grad_norm                      0.5\n",
      "grid_size                      (8, 8)\n",
      "input_size                     (128, 128)\n",
      "keep_top_k                     64\n",
      "load_model                     ../exp/alpr_det.pth\n",
      "load_pretrained                True\n",
      "log_file                       /home/hazen/data/SqueezeDet-PyTorch/exp/test_lpd_sqdet_trt_calib/training_logs.txt\n",
      "lr                             0.001\n",
      "master_batch_size              32\n",
      "mode                           eval\n",
      "momentum                       0.9\n",
      "negative_score_loss_weight     100.0\n",
      "nms_thresh                     0.4\n",
      "no_eval                        False\n",
      "not_cuda_benchmark             False\n",
      "num_anchors                    384\n",
      "num_classes                    1\n",
      "num_epochs                     100\n",
      "num_iters                      -1\n",
      "num_workers                    0\n",
      "positive_score_loss_weight     3.75\n",
      "print_interval                 20\n",
      "qat                            False\n",
      "rgb_mean                       [[[97.631615 98.70732  98.41285 ]]]\n",
      "rgb_std                        [[[52.766678 52.63513  52.348827]]]\n",
      "root_dir                       /home/hazen/data/SqueezeDet-PyTorch\n",
      "save_dir                       /home/hazen/data/SqueezeDet-PyTorch/exp/test_lpd_sqdet_trt_calib\n",
      "save_intervals                 1\n",
      "score_thresh                   0.5\n",
      "seed                           42\n",
      "stride                         16\n",
      "val_intervals                  1\n",
      "weight_decay                   0.0001\n",
      "loaded model ../exp/alpr_det.pth, epoch 268\n",
      "state_dict: \n",
      " dict_keys(['base.conv1.weight', 'base.conv1.bias', 'base.features.1.squeeze.weight', 'base.features.1.squeeze.bias', 'base.features.1.expand1x1.weight', 'base.features.1.expand1x1.bias', 'base.features.1.expand3x3.weight', 'base.features.1.expand3x3.bias', 'base.features.2.squeeze.weight', 'base.features.2.squeeze.bias', 'base.features.2.expand1x1.weight', 'base.features.2.expand1x1.bias', 'base.features.2.expand3x3.weight', 'base.features.2.expand3x3.bias', 'base.features.4.squeeze.weight', 'base.features.4.squeeze.bias', 'base.features.4.expand1x1.weight', 'base.features.4.expand1x1.bias', 'base.features.4.expand3x3.weight', 'base.features.4.expand3x3.bias', 'base.features.5.squeeze.weight', 'base.features.5.squeeze.bias', 'base.features.5.expand1x1.weight', 'base.features.5.expand1x1.bias', 'base.features.5.expand3x3.weight', 'base.features.5.expand3x3.bias', 'base.features.7.squeeze.weight', 'base.features.7.squeeze.bias', 'base.features.7.expand1x1.weight', 'base.features.7.expand1x1.bias', 'base.features.7.expand3x3.weight', 'base.features.7.expand3x3.bias', 'base.features.8.squeeze.weight', 'base.features.8.squeeze.bias', 'base.features.8.expand1x1.weight', 'base.features.8.expand1x1.bias', 'base.features.8.expand3x3.weight', 'base.features.8.expand3x3.bias', 'base.features.9.squeeze.weight', 'base.features.9.squeeze.bias', 'base.features.9.expand1x1.weight', 'base.features.9.expand1x1.bias', 'base.features.9.expand3x3.weight', 'base.features.9.expand3x3.bias', 'base.features.10.squeeze.weight', 'base.features.10.squeeze.bias', 'base.features.10.expand1x1.weight', 'base.features.10.expand1x1.bias', 'base.features.10.expand3x3.weight', 'base.features.10.expand3x3.bias', 'base.convdet.weight', 'base.convdet.bias'])\n",
      "Skip loading param base.convdet.weight, required shapetorch.Size([36, 768, 3, 3]), loaded shapetorch.Size([36, 512, 3, 3]).\n",
      "Param base.features.11.squeeze.weight not found in pre-trained model.\n",
      "Param base.features.11.squeeze.bias not found in pre-trained model.\n",
      "Param base.features.11.expand1x1.weight not found in pre-trained model.\n",
      "Param base.features.11.expand1x1.bias not found in pre-trained model.\n",
      "Param base.features.11.expand3x3.weight not found in pre-trained model.\n",
      "Param base.features.11.expand3x3.bias not found in pre-trained model.\n",
      "Param base.features.12.squeeze.weight not found in pre-trained model.\n",
      "Param base.features.12.squeeze.bias not found in pre-trained model.\n",
      "Param base.features.12.expand1x1.weight not found in pre-trained model.\n",
      "Param base.features.12.expand1x1.bias not found in pre-trained model.\n",
      "Param base.features.12.expand3x3.weight not found in pre-trained model.\n",
      "Param base.features.12.expand3x3.bias not found in pre-trained model.\n",
      "The model does not fully load the pre-trained weight.\n",
      "eval: [0/1530] | data 2.513s | net 0.010s\n",
      "eval: [20/1530] | data 0.006s | net 0.013s\n",
      "eval: [40/1530] | data 0.007s | net 0.016s\n",
      "eval: [60/1530] | data 0.010s | net 0.021s\n",
      "eval: [80/1530] | data 0.014s | net 0.036s\n",
      "eval: [100/1530] | data 0.028s | net 0.062s\n",
      "eval: [120/1530] | data 0.005s | net 0.014s\n",
      "eval: [140/1530] | data 0.028s | net 0.065s\n",
      "eval: [160/1530] | data 0.006s | net 0.016s\n",
      "eval: [180/1530] | data 0.002s | net 0.007s\n",
      "eval: [200/1530] | data 0.013s | net 0.034s\n",
      "eval: [220/1530] | data 0.044s | net 0.113s\n",
      "eval: [240/1530] | data 0.015s | net 0.038s\n",
      "eval: [260/1530] | data 0.007s | net 0.013s\n",
      "eval: [280/1530] | data 0.002s | net 0.007s\n",
      "eval: [300/1530] | data 0.009s | net 0.026s\n",
      "eval: [320/1530] | data 0.004s | net 0.012s\n",
      "eval: [340/1530] | data 0.010s | net 0.025s\n",
      "eval: [360/1530] | data 0.021s | net 0.055s\n",
      "eval: [380/1530] | data 0.008s | net 0.019s\n",
      "eval: [400/1530] | data 0.022s | net 0.044s\n",
      "eval: [420/1530] | data 0.003s | net 0.012s\n",
      "eval: [440/1530] | data 0.033s | net 0.077s\n",
      "eval: [460/1530] | data 0.002s | net 0.009s\n",
      "eval: [480/1530] | data 0.001s | net 0.007s\n",
      "eval: [500/1530] | data 0.008s | net 0.017s\n",
      "eval: [520/1530] | data 0.006s | net 0.015s\n",
      "eval: [540/1530] | data 0.003s | net 0.011s\n",
      "eval: [560/1530] | data 0.017s | net 0.032s\n",
      "eval: [580/1530] | data 0.032s | net 0.067s\n",
      "eval: [600/1530] | data 0.009s | net 0.025s\n",
      "eval: [620/1530] | data 0.002s | net 0.008s\n",
      "eval: [640/1530] | data 0.005s | net 0.013s\n",
      "eval: [660/1530] | data 0.004s | net 0.009s\n",
      "eval: [680/1530] | data 0.009s | net 0.014s\n",
      "eval: [700/1530] | data 0.009s | net 0.020s\n",
      "eval: [720/1530] | data 0.006s | net 0.012s\n",
      "eval: [740/1530] | data 0.007s | net 0.019s\n",
      "eval: [760/1530] | data 0.014s | net 0.019s\n",
      "eval: [780/1530] | data 0.006s | net 0.019s\n",
      "eval: [800/1530] | data 0.006s | net 0.015s\n",
      "eval: [820/1530] | data 0.012s | net 0.032s\n",
      "eval: [840/1530] | data 0.018s | net 0.023s\n",
      "eval: [860/1530] | data 0.026s | net 0.045s\n",
      "eval: [880/1530] | data 0.009s | net 0.010s\n",
      "eval: [900/1530] | data 0.003s | net 0.007s\n",
      "eval: [920/1530] | data 0.007s | net 0.023s\n",
      "eval: [940/1530] | data 0.017s | net 0.018s\n",
      "eval: [960/1530] | data 0.039s | net 0.028s\n",
      "eval: [980/1530] | data 0.043s | net 0.067s\n",
      "eval: [1000/1530] | data 0.068s | net 0.043s\n",
      "eval: [1020/1530] | data 0.044s | net 0.068s\n",
      "eval: [1040/1530] | data 0.027s | net 0.067s\n",
      "eval: [1060/1530] | data 0.006s | net 0.009s\n",
      "eval: [1080/1530] | data 0.003s | net 0.009s\n"
     ]
    }
   ],
   "source": [
    "from utils.misc import init_env\n",
    "init_env(cfg)\n",
    "from eval import eval\n",
    "eval(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model ../exp/alpr_det.pth, epoch 268\n",
      "state_dict: \n",
      " dict_keys(['base.conv1.weight', 'base.conv1.bias', 'base.features.1.squeeze.weight', 'base.features.1.squeeze.bias', 'base.features.1.expand1x1.weight', 'base.features.1.expand1x1.bias', 'base.features.1.expand3x3.weight', 'base.features.1.expand3x3.bias', 'base.features.2.squeeze.weight', 'base.features.2.squeeze.bias', 'base.features.2.expand1x1.weight', 'base.features.2.expand1x1.bias', 'base.features.2.expand3x3.weight', 'base.features.2.expand3x3.bias', 'base.features.4.squeeze.weight', 'base.features.4.squeeze.bias', 'base.features.4.expand1x1.weight', 'base.features.4.expand1x1.bias', 'base.features.4.expand3x3.weight', 'base.features.4.expand3x3.bias', 'base.features.5.squeeze.weight', 'base.features.5.squeeze.bias', 'base.features.5.expand1x1.weight', 'base.features.5.expand1x1.bias', 'base.features.5.expand3x3.weight', 'base.features.5.expand3x3.bias', 'base.features.7.squeeze.weight', 'base.features.7.squeeze.bias', 'base.features.7.expand1x1.weight', 'base.features.7.expand1x1.bias', 'base.features.7.expand3x3.weight', 'base.features.7.expand3x3.bias', 'base.features.8.squeeze.weight', 'base.features.8.squeeze.bias', 'base.features.8.expand1x1.weight', 'base.features.8.expand1x1.bias', 'base.features.8.expand3x3.weight', 'base.features.8.expand3x3.bias', 'base.features.9.squeeze.weight', 'base.features.9.squeeze.bias', 'base.features.9.expand1x1.weight', 'base.features.9.expand1x1.bias', 'base.features.9.expand3x3.weight', 'base.features.9.expand3x3.bias', 'base.features.10.squeeze.weight', 'base.features.10.squeeze.bias', 'base.features.10.expand1x1.weight', 'base.features.10.expand1x1.bias', 'base.features.10.expand3x3.weight', 'base.features.10.expand3x3.bias', 'base.convdet.weight', 'base.convdet.bias'])\n",
      "Skip loading param base.convdet.weight, required shapetorch.Size([36, 768, 3, 3]), loaded shapetorch.Size([36, 512, 3, 3]).\n",
      "Param base.features.11.squeeze.weight not found in pre-trained model.\n",
      "Param base.features.11.squeeze.bias not found in pre-trained model.\n",
      "Param base.features.11.expand1x1.weight not found in pre-trained model.\n",
      "Param base.features.11.expand1x1.bias not found in pre-trained model.\n",
      "Param base.features.11.expand3x3.weight not found in pre-trained model.\n",
      "Param base.features.11.expand3x3.bias not found in pre-trained model.\n",
      "Param base.features.12.squeeze.weight not found in pre-trained model.\n",
      "Param base.features.12.squeeze.bias not found in pre-trained model.\n",
      "Param base.features.12.expand1x1.weight not found in pre-trained model.\n",
      "Param base.features.12.expand1x1.bias not found in pre-trained model.\n",
      "Param base.features.12.expand3x3.weight not found in pre-trained model.\n",
      "Param base.features.12.expand3x3.bias not found in pre-trained model.\n",
      "The model does not fully load the pre-trained weight.\n",
      "SqueezeDetWithLoss(\n",
      "  (base): SqueezeDetBase(\n",
      "    (quant): QuantStub()\n",
      "    (dequant): DeQuantStub()\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (relu1): ReLU(inplace=True)\n",
      "    (features): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "      (1): Fire(\n",
      "        (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation_1): ReLU(inplace=True)\n",
      "        (activation_2): ReLU(inplace=True)\n",
      "        (activation_3): ReLU(inplace=True)\n",
      "        (float_functional_simple): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (2): Fire(\n",
      "        (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation_1): ReLU(inplace=True)\n",
      "        (activation_2): ReLU(inplace=True)\n",
      "        (activation_3): ReLU(inplace=True)\n",
      "        (float_functional_simple): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "      (4): Fire(\n",
      "        (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation_1): ReLU(inplace=True)\n",
      "        (activation_2): ReLU(inplace=True)\n",
      "        (activation_3): ReLU(inplace=True)\n",
      "        (float_functional_simple): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (5): Fire(\n",
      "        (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation_1): ReLU(inplace=True)\n",
      "        (activation_2): ReLU(inplace=True)\n",
      "        (activation_3): ReLU(inplace=True)\n",
      "        (float_functional_simple): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "      (7): Fire(\n",
      "        (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation_1): ReLU(inplace=True)\n",
      "        (activation_2): ReLU(inplace=True)\n",
      "        (activation_3): ReLU(inplace=True)\n",
      "        (float_functional_simple): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (8): Fire(\n",
      "        (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation_1): ReLU(inplace=True)\n",
      "        (activation_2): ReLU(inplace=True)\n",
      "        (activation_3): ReLU(inplace=True)\n",
      "        (float_functional_simple): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (9): Fire(\n",
      "        (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation_1): ReLU(inplace=True)\n",
      "        (activation_2): ReLU(inplace=True)\n",
      "        (activation_3): ReLU(inplace=True)\n",
      "        (float_functional_simple): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (10): Fire(\n",
      "        (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation_1): ReLU(inplace=True)\n",
      "        (activation_2): ReLU(inplace=True)\n",
      "        (activation_3): ReLU(inplace=True)\n",
      "        (float_functional_simple): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (11): Fire(\n",
      "        (squeeze): Conv2d(512, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (expand1x1): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (expand3x3): Conv2d(96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation_1): ReLU(inplace=True)\n",
      "        (activation_2): ReLU(inplace=True)\n",
      "        (activation_3): ReLU(inplace=True)\n",
      "        (float_functional_simple): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (12): Fire(\n",
      "        (squeeze): Conv2d(768, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (expand1x1): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (expand3x3): Conv2d(96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation_1): ReLU(inplace=True)\n",
      "        (activation_2): ReLU(inplace=True)\n",
      "        (activation_3): ReLU(inplace=True)\n",
      "        (float_functional_simple): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (convdet): Conv2d(768, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (resolver): PredictionResolver()\n",
      "  (loss): Loss(\n",
      "    (resolver): PredictionResolver()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if cfg.mode=='eval':\n",
    "        model = SqueezeDetWithLoss(cfg)\n",
    "        model = load_model(model, cfg.load_model, cfg)\n",
    "\n",
    "\n",
    "model.detect = True\n",
    "detector = Detector(model, cfg)\n",
    "print(detector.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import calib\n",
    "from pytorch_quantization.tensor_quant import QuantDescriptor\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO)  # Disable logging as they are too noisy in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_desc_input = QuantDescriptor(calib_method='histogram')\n",
    "quant_nn.QuantConv2d.set_default_quant_desc_input(quant_desc_input)\n",
    "quant_nn.QuantLinear.set_default_quant_desc_input(quant_desc_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_quantization import quant_modules\n",
    "quant_modules.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_stats(model, data_loader, num_batches):\n",
    "    \"\"\"Feed data to the network and collect statistic\"\"\"\n",
    "\n",
    "    # Enable calibrators\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.disable_quant()\n",
    "                module.enable_calib()\n",
    "            else:\n",
    "                module.disable()\n",
    "\n",
    "    # for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "    #     model(image.cuda())\n",
    "    #     if i >= num_batches:\n",
    "    #         break\n",
    "    # since lpr dataloader return a dict, so writing it in this way\n",
    "    for i, (batch) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "        for k in batch:\n",
    "                if 'image_meta' not in k:\n",
    "                    batch[k] = batch[k].to(device=\"cuda\", non_blocking=True)\n",
    "        x, _ = model(batch)\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "\n",
    "    # Disable calibrators\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.enable_quant()\n",
    "                module.disable_calib()\n",
    "            else:\n",
    "                module.enable()\n",
    "            \n",
    "def compute_amax(model, **kwargs):\n",
    "    # Load calib result\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                    module.load_calib_amax()\n",
    "                else:\n",
    "                    module.load_calib_amax(**kwargs)\n",
    "#             print(F\"{name:40}: {module}\")\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "\n",
    "from datasets.base import BaseDataset\n",
    "from utils.boxes import generate_anchors\n",
    "from PIL import Image\n",
    "from torchvision.datasets.folder import default_loader\n",
    "\n",
    "\n",
    "class LPR_CalibTRT(BaseDataset):\n",
    "    def __init__(self, phase, cfg):\n",
    "        super(LPR_CalibTRT, self).__init__(phase, cfg)\n",
    "\n",
    "        # self.input_size = (128, 128)  # (height, width), both dividable by 16\n",
    "        #this model for sadaia works with (256,256)\n",
    "        self.input_size = (256, 256)  # (height, width), both dividable by 16\n",
    "        self.class_names = ('0')\n",
    "        # real_filtered mean and std\n",
    "        # self.rgb_mean = np.array([94.87347, 96.89165, 94.70493], dtype=np.float32).reshape(1, 1, 3)\n",
    "        # self.rgb_std = np.array([53.869507, 53.936283, 55.2807], dtype=np.float32).reshape(1, 1, 3)\n",
    "        \n",
    "        # real_filtered plus all_sites_seatbelt mean and std\n",
    "        # self.rgb_mean = np.array([104.90631, 105.41336, 104.70162], dtype=np.float32).reshape(1, 1, 3)\n",
    "        # self.rgb_std = np.array([50.69564, 49.60443, 50.158844], dtype=np.float32).reshape(1, 1, 3)\n",
    "\n",
    "        self.rgb_mean = np.array([97.631615, 98.70732, 98.41285], dtype=np.float32).reshape(1, 1, 3)\n",
    "        self.rgb_std = np.array([52.766678, 52.63513, 52.348827], dtype=np.float32).reshape(1, 1, 3)\n",
    "\n",
    "        self.num_classes = len(self.class_names)\n",
    "        self.class_ids_dict = {cls_name: cls_id for cls_id, cls_name in enumerate(self.class_names)}\n",
    "\n",
    "        self.data_dir = os.path.join(cfg.data_dir, 'lpr_crop/merged_data')\n",
    "        self.sample_ids, self.sample_set_path = self.get_sample_ids()\n",
    "        self.grid_size = tuple(x // cfg.stride for x in self.input_size)  # anchors grid \n",
    "        # self.anchors_seed = np.array([[ 29, 17], [46, 32], [69, 52],\n",
    "        #                                 [109, 68], [84, 127], [155, 106], \n",
    "        #                                 [255, 145], [183, 215], [371, 221]], dtype=np.float32) ## real_filtered anchors\n",
    "        \n",
    "        # self.anchors_seed = np.array( [[ 32, 20], [ 61, 42], [ 59, 97],\n",
    "        #                                 [103, 66], [122, 114], [183, 96],\n",
    "        #                                 [160, 152], [211, 201], [343, 205]], dtype=np.float32) ## real_filtered plus all_sites_seatbelt anchors\n",
    "\n",
    "        # self.anchors_seed = np.array([[6, 5], [12, 10], [18, 10], [18, 18], [20, 24], [30, 15]], dtype=np.float32)\n",
    "        self.anchors_seed = np.array([[3, 3], [6, 5], [9, 5], [10, 9], [10, 12], [15, 8]], dtype=np.float32)\n",
    "        # self.anchors = generate_anchors(self.grid_size, self.input_size, self.anchors_seed)\n",
    "        self.anchors = torch.load(\"../exp/alpr_det_anchors_256_exp_8.pt\")\n",
    "        self.anchors_per_grid = self.anchors_seed.shape[0]\n",
    "        self.num_anchors = self.anchors.shape[0]\n",
    "\n",
    "        self.results_dir = os.path.join(cfg.save_dir, 'results')\n",
    "\n",
    "    def get_sample_ids(self):\n",
    "        #a dirty duct tape to load preprocessing of val phase but load dataset for train phase for trt calib\n",
    "        sample_set_name = 'train.txt' if self.phase == 'train' \\\n",
    "            else 'train.txt' if self.phase == 'val' \\\n",
    "            else 'trainval.txt' if self.phase == 'trainval' \\\n",
    "            else None\n",
    "\n",
    "        sample_ids_path = os.path.join(self.data_dir, sample_set_name)\n",
    "        with open(sample_ids_path, 'r') as fp:\n",
    "            sample_ids = fp.readlines()\n",
    "        sample_ids = tuple(x.strip() for x in sample_ids)\n",
    "\n",
    "        return sample_ids, sample_ids_path\n",
    "\n",
    "    def load_image(self, index):\n",
    "        image_id = self.sample_ids[index]\n",
    "        image_path = os.path.join(self.data_dir, 'images', image_id + '.png')\n",
    "        image = default_loader(image_path)\n",
    "        if image.mode == 'L':\n",
    "            image = image.convert('RGB')\n",
    "        image = np.array(image).astype(np.float32)\n",
    "        # image = skimage.io.imread(image_path).astype(np.float32)\n",
    "        return image, image_id\n",
    "\n",
    "    def load_annotations(self, index):\n",
    "        ann_id = self.sample_ids[index]\n",
    "        ann_path = os.path.join(self.data_dir, 'labels', ann_id + '.txt')\n",
    "        with open(ann_path, 'r') as fp:\n",
    "            annotations = fp.readlines()\n",
    "\n",
    "        annotations = [ann.strip().split(' ') for ann in annotations]\n",
    "        class_ids, boxes = [], []\n",
    "        for ann in annotations:\n",
    "            if ann[0] not in self.class_names:\n",
    "                continue\n",
    "            class_ids.append(self.class_ids_dict[ann[0]])\n",
    "            box = [float(x) for x in ann[4:8]]\n",
    "            # if box[2] <= 0:\n",
    "            #     box[2] = 0.00001\n",
    "            # if box[3] <= 0:\n",
    "            #     box[3] = 0.00001\n",
    "            boxes.append(box)\n",
    "\n",
    "        class_ids = np.array(class_ids, dtype=np.int16)\n",
    "        boxes = np.array(boxes, dtype=np.float32)\n",
    "        if len(boxes):\n",
    "            return class_ids, boxes\n",
    "        boxes = None\n",
    "        return class_ids, boxes\n",
    "\n",
    "    # ========================================\n",
    "    #                evaluation\n",
    "    # ========================================\n",
    "\n",
    "    def save_results(self, results):\n",
    "        txt_dir = os.path.join(self.results_dir, 'data')\n",
    "        os.makedirs(txt_dir, exist_ok=True)\n",
    "\n",
    "        for res in results:\n",
    "            txt_path = os.path.join(txt_dir, res['image_meta']['image_id'] + '.txt')\n",
    "            if 'class_ids' not in res:\n",
    "                with open(txt_path, 'w') as fp:\n",
    "                    fp.write('')\n",
    "                continue\n",
    "\n",
    "            num_boxes = len(res['class_ids'])\n",
    "            with open(txt_path, 'w') as fp:\n",
    "                for i in range(num_boxes):\n",
    "                    class_name = self.class_names[res['class_ids'][i]].lower()\n",
    "                    score = res['scores'][i]\n",
    "                    bbox = res['boxes'][i, :]\n",
    "                    line = '{} -1 -1 0 {:.2f} {:.2f} {:.2f} {:.2f} 0 0 0 0 0 0 0 {:.3f}\\n'.format(\n",
    "                            class_name, *bbox, score)\n",
    "                    fp.write(line)\n",
    "\n",
    "    def evaluate(self):\n",
    "        kitti_eval_tool_path = os.path.join(self.cfg.root_dir, 'src/utils/kitti-eval/cpp/evaluate_object')\n",
    "        cmd = '{} {} {} {} {}'.format(kitti_eval_tool_path,\n",
    "                                      os.path.join(self.data_dir),\n",
    "                                      self.sample_set_path,\n",
    "                                      self.results_dir,\n",
    "                                      len(self.sample_ids))\n",
    "\n",
    "        print (cmd)\n",
    "        status = subprocess.call(cmd, shell=True)\n",
    "\n",
    "        aps = {}\n",
    "        for class_name in self.class_names:\n",
    "            map_path = os.path.join(self.results_dir, 'stats_{}_ap.txt'.format(class_name.lower()))\n",
    "            if os.path.exists(map_path):\n",
    "                with open(map_path, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                _aps = [float(line.split('=')[1].strip()) for line in lines]\n",
    "            else:\n",
    "                _aps = [0., 0., 0.]\n",
    "\n",
    "            aps[class_name + '_easy'] = _aps[0]\n",
    "            aps[class_name + '_moderate'] = _aps[1]\n",
    "            aps[class_name + '_hard'] = _aps[2]\n",
    "\n",
    "        aps['mAP'] = sum(aps.values()) / len(aps)\n",
    "\n",
    "        return aps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpr_calib_dataset = LPR_CalibTRT('val', cfg)\n",
    "calib_data_loader = torch.utils.data.DataLoader(lpr_calib_dataset,\n",
    "                                                  batch_size=cfg.batch_size,\n",
    "                                                  num_workers=cfg.num_workers,\n",
    "                                                  pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14068\n"
     ]
    }
   ],
   "source": [
    "print (len(lpr_calib_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute stats and Calibrate on Different schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = int(len(lpr_calib_dataset)/cfg.batch_size)\n",
    "print (num_batches)\n",
    "\n",
    "detector.model.eval()\n",
    "detector.model.cuda()\n",
    "with torch.no_grad():\n",
    "    collect_stats(detector.model, calib_data_loader, num_batches=2000)\n",
    "    # compute_amax(detector.model, method=\"percentile\", percentile=99.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (detector.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'val'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets.lpr import LPR\n",
    "\n",
    "eval_dataset = LPR('val', cfg)\n",
    "eval_dataset.phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############  percentile calibration ######################\n",
      "torch.Size([1, 3, 128, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/hazen/data/SqueezeDet-PyTorch/src/lp_det_sqdet_trt_calib.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hazen/data/SqueezeDet-PyTorch/src/lp_det_sqdet_trt_calib.ipynb#Y100sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     compute_amax(detector\u001b[39m.\u001b[39mmodel, method\u001b[39m=\u001b[39mmethod)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hazen/data/SqueezeDet-PyTorch/src/lp_det_sqdet_trt_calib.ipynb#Y100sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m update_exp_dir(cfg, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrt_calib_lpd_sadaia_sqdet_\u001b[39m\u001b[39m{\u001b[39;00mmethod\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hazen/data/SqueezeDet-PyTorch/src/lp_det_sqdet_trt_calib.ipynb#Y100sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m results \u001b[39m=\u001b[39m detector\u001b[39m.\u001b[39;49mdetect_dataset(eval_dataset, cfg)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hazen/data/SqueezeDet-PyTorch/src/lp_det_sqdet_trt_calib.ipynb#Y100sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m eval_dataset\u001b[39m.\u001b[39msave_results(results)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hazen/data/SqueezeDet-PyTorch/src/lp_det_sqdet_trt_calib.ipynb#Y100sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m aps \u001b[39m=\u001b[39m eval_dataset\u001b[39m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/data/SqueezeDet-PyTorch/src/engine/detector.py:74\u001b[0m, in \u001b[0;36mDetector.detect_dataset\u001b[0;34m(self, dataset, cfg)\u001b[0m\n\u001b[1;32m     72\u001b[0m data_timer\u001b[39m.\u001b[39mupdate(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m end)\n\u001b[1;32m     73\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 74\u001b[0m results\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdetect(batch))\n\u001b[1;32m     76\u001b[0m net_timer\u001b[39m.\u001b[39mupdate(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m end)\n\u001b[1;32m     77\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/data/SqueezeDet-PyTorch/src/engine/detector.py:26\u001b[0m, in \u001b[0;36mDetector.detect\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdetect\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m---> 26\u001b[0m     dets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(batch)\n\u001b[1;32m     28\u001b[0m     results \u001b[39m=\u001b[39m []\n\u001b[1;32m     29\u001b[0m     batch_size \u001b[39m=\u001b[39m dets[\u001b[39m'\u001b[39m\u001b[39mclass_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/ocr-qat-clone/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/data/SqueezeDet-PyTorch/src/model/squeezedet.py:350\u001b[0m, in \u001b[0;36mSqueezeDetWithLoss.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m--> 350\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase(batch[\u001b[39m'\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    351\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdetect:\n\u001b[1;32m    352\u001b[0m         loss, loss_stats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(pred, batch[\u001b[39m'\u001b[39m\u001b[39mgt\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/ocr-qat-clone/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/data/SqueezeDet-PyTorch/src/model/squeezedet.py:204\u001b[0m, in \u001b[0;36mSqueezeDetBase.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    202\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquant(x)\n\u001b[1;32m    203\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39march\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39msqueezedet\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m    205\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu1(x)\n\u001b[1;32m    206\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/ocr-qat-clone/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/ocr-qat-clone/lib/python3.8/site-packages/torch/nn/modules/conv.py:443\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 443\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/ocr-qat-clone/lib/python3.8/site-packages/torch/nn/modules/conv.py:439\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    436\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    437\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    438\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 439\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    440\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for method in [\"percentile\", \"mse\", \"entropy\"]:\n",
    "        percentile = 99.99\n",
    "        print(F\" ############  {method} calibration ######################\")\n",
    "        if(method == 'percentile'):\n",
    "            compute_amax(detector.model, method=method, percentile = percentile)\n",
    "        else:\n",
    "            compute_amax(detector.model, method=method)\n",
    "        update_exp_dir(cfg, f\"trt_calib_lpd_sadaia_sqdet_{method}\")\n",
    "        results = detector.detect_dataset(eval_dataset, cfg)\n",
    "        eval_dataset.save_results(results)\n",
    "        aps = eval_dataset.evaluate()\n",
    "        for k, v in aps.items():\n",
    "                print('{:<20} {:.3f}'.format(k, v))\n",
    "        torch.save(detector.model.state_dict(), os.path.join(cfg.save_dir, f\"quant_lp_squeezedet_sadaia_{method}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (detector.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add scales to the quantized state dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = torch.load(\"../exp/trt_calib_lpd_sadaia_sqdet_percentile/quant_lp_squeezedet_sadaia_percentile.pth\")\n",
    "print (dict.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (alpr)\n",
    "def get_scales_input_per_tensor(amax, _num_bits = 8 , _unsigned = False):\n",
    "    bound = (1 << (_num_bits - 1 + int(_unsigned))) - 1    \n",
    "    zero_point_per_tensor = 0 #since symmertric quant\n",
    "    scale_per_tensor = amax / bound\n",
    "    return scale_per_tensor, zero_point_per_tensor\n",
    "\n",
    "def get_scales_weight_per_channel(amax, _num_bits = 8, _unsigned = False):\n",
    "    bound = (1 << (_num_bits - 1 + int(_unsigned))) - 1\n",
    "    amax_sequeeze = amax.squeeze().detach()\n",
    "    scale_per_channel =  amax_sequeeze/bound\n",
    "    zero_point_per_channel = torch.zeros_like(scale_per_channel, dtype=torch.int32).data\n",
    "    quant_axis = list(amax.shape).index(list(amax_sequeeze.shape)[0])\n",
    "    return scale_per_channel, zero_point_per_channel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['base.conv1.weight', 'base.conv1.bias', 'base.conv1._input_quantizer._amax.scales', 'base.conv1._input_quantizer._amax', 'base.conv1._weight_quantizer._amax.scales', 'base.conv1._weight_quantizer._amax', 'base.features.1.squeeze.weight', 'base.features.1.squeeze.bias', 'base.features.1.squeeze._input_quantizer._amax.scales', 'base.features.1.squeeze._input_quantizer._amax', 'base.features.1.squeeze._weight_quantizer._amax.scales', 'base.features.1.squeeze._weight_quantizer._amax', 'base.features.1.expand1x1.weight', 'base.features.1.expand1x1.bias', 'base.features.1.expand1x1._input_quantizer._amax.scales', 'base.features.1.expand1x1._input_quantizer._amax', 'base.features.1.expand1x1._weight_quantizer._amax.scales', 'base.features.1.expand1x1._weight_quantizer._amax', 'base.features.1.expand3x3.weight', 'base.features.1.expand3x3.bias', 'base.features.1.expand3x3._input_quantizer._amax.scales', 'base.features.1.expand3x3._input_quantizer._amax', 'base.features.1.expand3x3._weight_quantizer._amax.scales', 'base.features.1.expand3x3._weight_quantizer._amax', 'base.features.2.squeeze.weight', 'base.features.2.squeeze.bias', 'base.features.2.squeeze._input_quantizer._amax.scales', 'base.features.2.squeeze._input_quantizer._amax', 'base.features.2.squeeze._weight_quantizer._amax.scales', 'base.features.2.squeeze._weight_quantizer._amax', 'base.features.2.expand1x1.weight', 'base.features.2.expand1x1.bias', 'base.features.2.expand1x1._input_quantizer._amax.scales', 'base.features.2.expand1x1._input_quantizer._amax', 'base.features.2.expand1x1._weight_quantizer._amax.scales', 'base.features.2.expand1x1._weight_quantizer._amax', 'base.features.2.expand3x3.weight', 'base.features.2.expand3x3.bias', 'base.features.2.expand3x3._input_quantizer._amax.scales', 'base.features.2.expand3x3._input_quantizer._amax', 'base.features.2.expand3x3._weight_quantizer._amax.scales', 'base.features.2.expand3x3._weight_quantizer._amax', 'base.features.4.squeeze.weight', 'base.features.4.squeeze.bias', 'base.features.4.squeeze._input_quantizer._amax.scales', 'base.features.4.squeeze._input_quantizer._amax', 'base.features.4.squeeze._weight_quantizer._amax.scales', 'base.features.4.squeeze._weight_quantizer._amax', 'base.features.4.expand1x1.weight', 'base.features.4.expand1x1.bias', 'base.features.4.expand1x1._input_quantizer._amax.scales', 'base.features.4.expand1x1._input_quantizer._amax', 'base.features.4.expand1x1._weight_quantizer._amax.scales', 'base.features.4.expand1x1._weight_quantizer._amax', 'base.features.4.expand3x3.weight', 'base.features.4.expand3x3.bias', 'base.features.4.expand3x3._input_quantizer._amax.scales', 'base.features.4.expand3x3._input_quantizer._amax', 'base.features.4.expand3x3._weight_quantizer._amax.scales', 'base.features.4.expand3x3._weight_quantizer._amax', 'base.features.5.squeeze.weight', 'base.features.5.squeeze.bias', 'base.features.5.squeeze._input_quantizer._amax.scales', 'base.features.5.squeeze._input_quantizer._amax', 'base.features.5.squeeze._weight_quantizer._amax.scales', 'base.features.5.squeeze._weight_quantizer._amax', 'base.features.5.expand1x1.weight', 'base.features.5.expand1x1.bias', 'base.features.5.expand1x1._input_quantizer._amax.scales', 'base.features.5.expand1x1._input_quantizer._amax', 'base.features.5.expand1x1._weight_quantizer._amax.scales', 'base.features.5.expand1x1._weight_quantizer._amax', 'base.features.5.expand3x3.weight', 'base.features.5.expand3x3.bias', 'base.features.5.expand3x3._input_quantizer._amax.scales', 'base.features.5.expand3x3._input_quantizer._amax', 'base.features.5.expand3x3._weight_quantizer._amax.scales', 'base.features.5.expand3x3._weight_quantizer._amax', 'base.features.7.squeeze.weight', 'base.features.7.squeeze.bias', 'base.features.7.squeeze._input_quantizer._amax.scales', 'base.features.7.squeeze._input_quantizer._amax', 'base.features.7.squeeze._weight_quantizer._amax.scales', 'base.features.7.squeeze._weight_quantizer._amax', 'base.features.7.expand1x1.weight', 'base.features.7.expand1x1.bias', 'base.features.7.expand1x1._input_quantizer._amax.scales', 'base.features.7.expand1x1._input_quantizer._amax', 'base.features.7.expand1x1._weight_quantizer._amax.scales', 'base.features.7.expand1x1._weight_quantizer._amax', 'base.features.7.expand3x3.weight', 'base.features.7.expand3x3.bias', 'base.features.7.expand3x3._input_quantizer._amax.scales', 'base.features.7.expand3x3._input_quantizer._amax', 'base.features.7.expand3x3._weight_quantizer._amax.scales', 'base.features.7.expand3x3._weight_quantizer._amax', 'base.features.8.squeeze.weight', 'base.features.8.squeeze.bias', 'base.features.8.squeeze._input_quantizer._amax.scales', 'base.features.8.squeeze._input_quantizer._amax', 'base.features.8.squeeze._weight_quantizer._amax.scales', 'base.features.8.squeeze._weight_quantizer._amax', 'base.features.8.expand1x1.weight', 'base.features.8.expand1x1.bias', 'base.features.8.expand1x1._input_quantizer._amax.scales', 'base.features.8.expand1x1._input_quantizer._amax', 'base.features.8.expand1x1._weight_quantizer._amax.scales', 'base.features.8.expand1x1._weight_quantizer._amax', 'base.features.8.expand3x3.weight', 'base.features.8.expand3x3.bias', 'base.features.8.expand3x3._input_quantizer._amax.scales', 'base.features.8.expand3x3._input_quantizer._amax', 'base.features.8.expand3x3._weight_quantizer._amax.scales', 'base.features.8.expand3x3._weight_quantizer._amax', 'base.features.9.squeeze.weight', 'base.features.9.squeeze.bias', 'base.features.9.squeeze._input_quantizer._amax.scales', 'base.features.9.squeeze._input_quantizer._amax', 'base.features.9.squeeze._weight_quantizer._amax.scales', 'base.features.9.squeeze._weight_quantizer._amax', 'base.features.9.expand1x1.weight', 'base.features.9.expand1x1.bias', 'base.features.9.expand1x1._input_quantizer._amax.scales', 'base.features.9.expand1x1._input_quantizer._amax', 'base.features.9.expand1x1._weight_quantizer._amax.scales', 'base.features.9.expand1x1._weight_quantizer._amax', 'base.features.9.expand3x3.weight', 'base.features.9.expand3x3.bias', 'base.features.9.expand3x3._input_quantizer._amax.scales', 'base.features.9.expand3x3._input_quantizer._amax', 'base.features.9.expand3x3._weight_quantizer._amax.scales', 'base.features.9.expand3x3._weight_quantizer._amax', 'base.features.10.squeeze.weight', 'base.features.10.squeeze.bias', 'base.features.10.squeeze._input_quantizer._amax.scales', 'base.features.10.squeeze._input_quantizer._amax', 'base.features.10.squeeze._weight_quantizer._amax.scales', 'base.features.10.squeeze._weight_quantizer._amax', 'base.features.10.expand1x1.weight', 'base.features.10.expand1x1.bias', 'base.features.10.expand1x1._input_quantizer._amax.scales', 'base.features.10.expand1x1._input_quantizer._amax', 'base.features.10.expand1x1._weight_quantizer._amax.scales', 'base.features.10.expand1x1._weight_quantizer._amax', 'base.features.10.expand3x3.weight', 'base.features.10.expand3x3.bias', 'base.features.10.expand3x3._input_quantizer._amax.scales', 'base.features.10.expand3x3._input_quantizer._amax', 'base.features.10.expand3x3._weight_quantizer._amax.scales', 'base.features.10.expand3x3._weight_quantizer._amax', 'base.convdet.weight', 'base.convdet.bias', 'base.convdet._input_quantizer._amax.scales', 'base.convdet._input_quantizer._amax', 'base.convdet._weight_quantizer._amax.scales', 'base.convdet._weight_quantizer._amax'])\n"
     ]
    }
   ],
   "source": [
    "lp_det_sadaia_dict_with_scales = {}\n",
    "\n",
    "for key in dict:\n",
    "    if key.find('_input_quantizer._amax') != -1:\n",
    "        amax = dict[key]\n",
    "        scales, zero_point = get_scales_input_per_tensor(amax)\n",
    "        new_key = key + '.scales'\n",
    "        lp_det_sadaia_dict_with_scales[new_key] = scales\n",
    "    elif key.find('_weight_quantizer._amax') != -1:\n",
    "        amax = dict[key]\n",
    "        scales, zero_point = get_scales_weight_per_channel(amax)\n",
    "        new_key = key + '.scales'\n",
    "        lp_det_sadaia_dict_with_scales[new_key] = scales\n",
    "    \n",
    "    lp_det_sadaia_dict_with_scales[key] = dict[key]\n",
    "\n",
    "print (lp_det_sadaia_dict_with_scales.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lp_det_sadaia_dict_with_scales, \"../exp/quant_lp_det_sadaia_with_scales.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 800, 3)\n",
      "81.0\n",
      "(600, 800, 3)\n",
      "90.0\n",
      "(600, 800, 3)\n",
      "139.0\n",
      "(600, 800, 3)\n",
      "150.0\n",
      "(600, 800, 3)\n",
      "85.0\n",
      "(600, 800, 3)\n",
      "129.0\n",
      "(600, 800, 3)\n",
      "93.0\n",
      "(600, 800, 3)\n",
      "118.0\n",
      "(600, 800, 3)\n",
      "93.0\n",
      "(600, 800, 3)\n",
      "132.0\n",
      "(600, 800, 3)\n",
      "112.0\n",
      "(600, 800, 3)\n",
      "73.0\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from os import *\n",
    "from utils import image\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "images_dir = \"/home/hazen/data/SqueezeDet-PyTorch/data/test_trt\"\n",
    "convdet_dir = \"/home/hazen/data/SqueezeDet-PyTorch/data/wrong_convdet/\"\n",
    "image_paths = glob(path.join(images_dir,\"*\"))\n",
    "image_paths.sort()\n",
    "for i, path in enumerate(image_paths):\n",
    "        img = default_loader(path)\n",
    "        # if img.mode == 'L':\n",
    "        #         img = img.convert('RGB')\n",
    "        print(np.array(img).shape)\n",
    "        trt_xl = np.load(convdet_dir+path.split(\"/\")[-1].split(\".\")[0]+\".npy\")\n",
    "        x = np.array(img)-trt_xl\n",
    "        print(np.max(x))\n",
    "        # img = transforms.ToTensor()(img)\n",
    "        # img = transforms.Resize((256,256),interpolation=InterpolationMode.NEAREST) (img)\n",
    "        # img = transforms.Grayscale(num_output_channels=3) (img)\n",
    "        \n",
    "        # trt_xl = np.load(convdet_dir+path.split(\"/\")[-1].split(\".\")[0]+\".npy\")\n",
    "        # x = img.numpy()-trt_xl\n",
    "        # print(np.max(x))\n",
    "        # # img = img.permute(2, 0, 1)\n",
    "        # # print(img.shape)\n",
    "        # img = img.reshape(1,3,256,256)\n",
    "        # trt_xl = trt_xl.reshape(1,3,256,256)\n",
    "        # x = img.numpy()-trt_xl\n",
    "        # # print(\"2\",np.max(x))\n",
    "        # img = img.cuda()\n",
    "        \n",
    "        # x,xl = model.base(img)\n",
    "        # # xl = xl.reshape(36,16,16)\n",
    "        # xl = xl.to('cpu').detach().numpy()\n",
    "        # # print(\"TRT: \",trt_xl)\n",
    "        # # print(\"PyTorch: \", xl)\n",
    "        # #print(np.max(xl-trt_xl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.7525e-08, -6.0345e-08, -7.5839e-08,  ...,  6.9797e-08,\n",
      "            1.1774e-07, -1.6262e-08],\n",
      "          [ 8.8885e-08,  1.5105e-07, -7.9799e-08,  ...,  9.6999e-08,\n",
      "            1.0943e-07,  8.8961e-08],\n",
      "          [ 1.8245e-08,  7.4072e-08,  5.2735e-08,  ...,  1.0661e-07,\n",
      "            3.4629e-08,  6.4640e-08],\n",
      "          ...,\n",
      "          [ 1.1090e-08, -2.8018e-08, -8.3166e-08,  ...,  2.2642e-08,\n",
      "            1.9277e-07,  3.1397e-08],\n",
      "          [ 6.4179e-09,  1.2052e-07,  5.0777e-08,  ...,  9.7679e-08,\n",
      "            1.3850e-07,  8.2770e-08],\n",
      "          [ 5.0847e-09, -5.7183e-08,  4.0392e-08,  ...,  4.4975e-08,\n",
      "            6.7946e-08, -8.4506e-09]],\n",
      "\n",
      "         [[-2.8623e+00, -2.8623e+00, -2.8623e+00,  ..., -2.8623e+00,\n",
      "           -2.8623e+00, -2.8623e+00],\n",
      "          [-2.8623e+00, -2.8623e+00, -2.8623e+00,  ..., -2.8623e+00,\n",
      "           -2.8623e+00, -2.8623e+00],\n",
      "          [-2.8623e+00, -2.8623e+00, -2.8623e+00,  ..., -2.8623e+00,\n",
      "           -2.8623e+00, -2.8623e+00],\n",
      "          ...,\n",
      "          [-2.8623e+00, -2.8623e+00, -2.8623e+00,  ..., -2.8623e+00,\n",
      "           -2.8623e+00, -2.8623e+00],\n",
      "          [-2.8623e+00, -2.8623e+00, -2.8623e+00,  ..., -2.8623e+00,\n",
      "           -2.8623e+00, -2.8623e+00],\n",
      "          [-2.8623e+00, -2.8623e+00, -2.8623e+00,  ..., -2.8623e+00,\n",
      "           -2.8623e+00, -2.8623e+00]],\n",
      "\n",
      "         [[-4.4460e-03, -4.4459e-03, -4.4459e-03,  ..., -4.4460e-03,\n",
      "           -4.4461e-03, -4.4460e-03],\n",
      "          [-4.4461e-03, -4.4458e-03, -4.4461e-03,  ..., -4.4461e-03,\n",
      "           -4.4460e-03, -4.4459e-03],\n",
      "          [-4.4461e-03, -4.4460e-03, -4.4460e-03,  ..., -4.4462e-03,\n",
      "           -4.4461e-03, -4.4460e-03],\n",
      "          ...,\n",
      "          [-4.4461e-03, -4.4459e-03, -4.4459e-03,  ..., -4.4460e-03,\n",
      "           -4.4461e-03, -4.4460e-03],\n",
      "          [-4.4461e-03, -4.4460e-03, -4.4460e-03,  ..., -4.4460e-03,\n",
      "           -4.4460e-03, -4.4461e-03],\n",
      "          [-4.4461e-03, -4.4461e-03, -4.4462e-03,  ..., -4.4462e-03,\n",
      "           -4.4462e-03, -4.4462e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.4360e-02, -2.4360e-02, -2.4360e-02,  ..., -2.4360e-02,\n",
      "           -2.4360e-02, -2.4360e-02],\n",
      "          [-2.4360e-02, -2.4360e-02, -2.4360e-02,  ..., -2.4360e-02,\n",
      "           -2.4360e-02, -2.4360e-02],\n",
      "          [-2.4360e-02, -2.4360e-02, -2.4360e-02,  ..., -2.4360e-02,\n",
      "           -2.4360e-02, -2.4360e-02],\n",
      "          ...,\n",
      "          [-2.4360e-02, -2.4360e-02, -2.4360e-02,  ..., -2.4360e-02,\n",
      "           -2.4360e-02, -2.4360e-02],\n",
      "          [-2.4360e-02, -2.4360e-02, -2.4360e-02,  ..., -2.4360e-02,\n",
      "           -2.4360e-02, -2.4360e-02],\n",
      "          [-2.4360e-02, -2.4360e-02, -2.4360e-02,  ..., -2.4360e-02,\n",
      "           -2.4360e-02, -2.4360e-02]],\n",
      "\n",
      "         [[ 7.6033e-02,  7.6033e-02,  7.6033e-02,  ...,  7.6033e-02,\n",
      "            7.6033e-02,  7.6033e-02],\n",
      "          [ 7.6033e-02,  7.6032e-02,  7.6032e-02,  ...,  7.6032e-02,\n",
      "            7.6033e-02,  7.6033e-02],\n",
      "          [ 7.6032e-02,  7.6032e-02,  7.6032e-02,  ...,  7.6032e-02,\n",
      "            7.6032e-02,  7.6033e-02],\n",
      "          ...,\n",
      "          [ 7.6032e-02,  7.6032e-02,  7.6032e-02,  ...,  7.6032e-02,\n",
      "            7.6032e-02,  7.6033e-02],\n",
      "          [ 7.6032e-02,  7.6032e-02,  7.6032e-02,  ...,  7.6032e-02,\n",
      "            7.6032e-02,  7.6033e-02],\n",
      "          [ 7.6032e-02,  7.6032e-02,  7.6033e-02,  ...,  7.6033e-02,\n",
      "            7.6033e-02,  7.6033e-02]],\n",
      "\n",
      "         [[ 2.0068e-01,  2.0068e-01,  2.0068e-01,  ...,  2.0068e-01,\n",
      "            2.0068e-01,  2.0068e-01],\n",
      "          [ 2.0068e-01,  2.0068e-01,  2.0068e-01,  ...,  2.0068e-01,\n",
      "            2.0068e-01,  2.0068e-01],\n",
      "          [ 2.0068e-01,  2.0068e-01,  2.0068e-01,  ...,  2.0068e-01,\n",
      "            2.0068e-01,  2.0068e-01],\n",
      "          ...,\n",
      "          [ 2.0068e-01,  2.0068e-01,  2.0068e-01,  ...,  2.0068e-01,\n",
      "            2.0068e-01,  2.0068e-01],\n",
      "          [ 2.0068e-01,  2.0068e-01,  2.0068e-01,  ...,  2.0068e-01,\n",
      "            2.0068e-01,  2.0068e-01],\n",
      "          [ 2.0068e-01,  2.0068e-01,  2.0068e-01,  ...,  2.0068e-01,\n",
      "            2.0068e-01,  2.0068e-01]]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "x = torch.rand(size=(1,3,256,256)).to(\"cuda\")\n",
    "x,xl = model.base(x)\n",
    "print(xl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SqueezeDetBase(\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (relu1): ReLU(inplace=True)\n",
       "  (features): Sequential(\n",
       "    (0): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (1): Fire(\n",
       "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (activation_1): ReLU(inplace=True)\n",
       "      (activation_2): ReLU(inplace=True)\n",
       "      (activation_3): ReLU(inplace=True)\n",
       "      (float_functional_simple): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): Fire(\n",
       "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (activation_1): ReLU(inplace=True)\n",
       "      (activation_2): ReLU(inplace=True)\n",
       "      (activation_3): ReLU(inplace=True)\n",
       "      (float_functional_simple): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (4): Fire(\n",
       "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (activation_1): ReLU(inplace=True)\n",
       "      (activation_2): ReLU(inplace=True)\n",
       "      (activation_3): ReLU(inplace=True)\n",
       "      (float_functional_simple): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (5): Fire(\n",
       "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (activation_1): ReLU(inplace=True)\n",
       "      (activation_2): ReLU(inplace=True)\n",
       "      (activation_3): ReLU(inplace=True)\n",
       "      (float_functional_simple): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (7): Fire(\n",
       "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (activation_1): ReLU(inplace=True)\n",
       "      (activation_2): ReLU(inplace=True)\n",
       "      (activation_3): ReLU(inplace=True)\n",
       "      (float_functional_simple): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (8): Fire(\n",
       "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (activation_1): ReLU(inplace=True)\n",
       "      (activation_2): ReLU(inplace=True)\n",
       "      (activation_3): ReLU(inplace=True)\n",
       "      (float_functional_simple): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (9): Fire(\n",
       "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (activation_1): ReLU(inplace=True)\n",
       "      (activation_2): ReLU(inplace=True)\n",
       "      (activation_3): ReLU(inplace=True)\n",
       "      (float_functional_simple): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (activation_1): ReLU(inplace=True)\n",
       "      (activation_2): ReLU(inplace=True)\n",
       "      (activation_3): ReLU(inplace=True)\n",
       "      (float_functional_simple): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (11): Fire(\n",
       "      (squeeze): Conv2d(512, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand3x3): Conv2d(96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (activation_1): ReLU(inplace=True)\n",
       "      (activation_2): ReLU(inplace=True)\n",
       "      (activation_3): ReLU(inplace=True)\n",
       "      (float_functional_simple): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (12): Fire(\n",
       "      (squeeze): Conv2d(768, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand3x3): Conv2d(96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (activation_1): ReLU(inplace=True)\n",
       "      (activation_2): ReLU(inplace=True)\n",
       "      (activation_3): ReLU(inplace=True)\n",
       "      (float_functional_simple): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (convdet): Conv2d(768, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ocr-qat-clone')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed7a881ff41a78f071bc8310ea2e885dbdfc33c63737541739e546362a580f91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
